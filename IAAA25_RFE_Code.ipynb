{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VLGD1q7Y3R4P"
   },
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "srhHkTRiDpy_"
   },
   "outputs": [],
   "source": [
    "!uv add \"numpy==2.0.2\" \"pandas==2.2.2\" \"matplotlib==3.10.0\" \"scipy==1.15.3\" \"scikit-learn==1.6.1\" \"openpyxl==3.1.5\" \"imbalanced-learn==0.13.0\" \"xgboost==2.1.4\" \"lightgbm==4.5.0\" \"tensorflow==2.18.0\" \"keras==3.8.0\" \"PyWavelets==1.8.0\" \"ipykernel==6.17.1\" \"tsassure-feature==0.7\" gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6nar8uBQSseP"
   },
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "\n",
    "packages = [\n",
    "    \"numpy\", \"pandas\", \"matplotlib\", \"scipy\", \"scikit-learn\",\n",
    "    \"openpyxl\", \"imbalanced-learn\", \"xgboost\", \"lightgbm\",\n",
    "    \"tensorflow\", \"keras\", \"PyWavelets\", \"ipykernel\", \"tsassure-feature\"\n",
    "]\n",
    "\n",
    "for p in packages:\n",
    "    try:\n",
    "        print(f\"{p}: {pkg_resources.get_distribution(p).version}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{p}: Not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIRoemC73WvP"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mCSlvVgfjt_D",
    "outputId": "596f097b-812e-40b0-8078-a7694aa4791f"
   },
   "outputs": [],
   "source": [
    "\n",
    "!rm -rf /content/dataset\n",
    "!gdown --folder https://drive.google.com/drive/folders/1BeZmzXCMpUWKL9zFrNoWoJsvkiBwpfQp?usp=sharing -O ./content/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "YM0CwfQq0Oy-",
    "outputId": "b5428197-b6d3-4a98-fc92-eb9cd4b9c6bf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "file_path_1 = \"./content/dataset/Train-set_1 - draw_input.xlsx\"\n",
    "file_path_2 = \"./content/dataset/Train-set_2 - draw_input.xlsx\"\n",
    "\n",
    "def load_data(path):\n",
    "    df = pd.read_excel(path)\n",
    "    if 'Time' in df.columns:\n",
    "        df['Time'] = pd.to_datetime(df['Time'], errors='coerce')\n",
    "        df.dropna(subset=['Time'], inplace=True)\n",
    "    return df[['Time', 'Temperature', 'Humidity']].dropna()\n",
    "\n",
    "df_normal = load_data(file_path_1)\n",
    "df_faulty = load_data(file_path_2)\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(df_normal['Time'], df_normal['Temperature'], color='black')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(df_normal['Time'], df_normal['Humidity'], color='black')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(df_faulty['Time'], df_faulty['Temperature'], color='black')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(df_faulty['Time'], df_faulty['Humidity'], color='black')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnO9de9t9L4L"
   },
   "source": [
    "# RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JtLhClDWDzkQ"
   },
   "outputs": [],
   "source": [
    "# CELL 1: Define the SINGLE-INPUT RobustFeatureExtractor Class (with Speed Change Stats)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"RuntimeWarning: All-NaN slice encountered\")\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning, message='invalid value encountered in divide')\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning, message='divide by zero encountered in divide')\n",
    "\n",
    "# ==============================================================================\n",
    "# == SINGLE-INPUT RobustFeatureExtractor Class Definition (v2) ==\n",
    "# ==============================================================================\n",
    "\n",
    "class RobustFeatureExtractor:\n",
    "    \"\"\"\n",
    "    MODIFIED version 2: Extracts features from a SINGLE input file.\n",
    "    Includes rolling stats on speed change features.\n",
    "    Prevents label leakage. Offers lags, multi-window rolling stats, time features, EWMA.\n",
    "    Correlation features are removed.\n",
    "    \"\"\"\n",
    "    # --- Type Hinting ---\n",
    "    data_main: pd.DataFrame\n",
    "    original_main_columns: list\n",
    "    numeric_cols_main: list\n",
    "    time_col_name: str\n",
    "    rolling_windows: list[int]\n",
    "    num_lags: int\n",
    "    time_col_is_datetime: bool\n",
    "\n",
    "    def __init__(self, input_file: str,\n",
    "                 time_col_name: str = \"Time\",\n",
    "                 rolling_windows: list[int] = [5, 15],\n",
    "                 num_lags: int = 3):\n",
    "        \"\"\" Initializes the Single-Input RobustFeatureExtractor. \"\"\"\n",
    "        self.time_col_name = time_col_name\n",
    "        self.rolling_windows = sorted([max(2, w) for w in rolling_windows if isinstance(w, int) and w > 1])\n",
    "        if not self.rolling_windows: self.rolling_windows = [5]\n",
    "        self.num_lags = max(0, num_lags)\n",
    "        if self.num_lags > 10: print(f\"Warning: num_lags ({self.num_lags}) is large.\")\n",
    "\n",
    "        # --- Load Data ---\n",
    "        try:\n",
    "            df_main_raw = pd.read_excel(input_file)\n",
    "            self.original_main_columns = df_main_raw.columns.tolist()\n",
    "        except FileNotFoundError as e: raise SystemExit(f\"❌ ERROR: File not found: {e}\")\n",
    "        except Exception as e: raise SystemExit(f\"❌ ERROR: Error processing Excel: {e}\")\n",
    "\n",
    "        # --- Process Main Data ---\n",
    "        self.data_main = pd.DataFrame(index=df_main_raw.index)\n",
    "        numeric_cols_main_temp = []\n",
    "        self.time_col_is_datetime = False\n",
    "        print(f\"\\nProcessing {input_file} (main data)...\")\n",
    "        if 'Label' not in self.original_main_columns: print(f\"Warning: 'Label' column not found.\")\n",
    "\n",
    "        for col in self.original_main_columns:\n",
    "            if col == self.time_col_name:\n",
    "                try:\n",
    "                    self.data_main[col] = pd.to_datetime(df_main_raw[col], errors='coerce')\n",
    "                    if self.data_main[col].notna().sum() > 0.5 * len(df_main_raw):\n",
    "                        self.time_col_is_datetime = True; print(f\"   Column '{self.time_col_name}' processed as datetime.\")\n",
    "                    else: self.data_main[col] = df_main_raw[col]; print(f\"   Warning: Low valid datetime %.\")\n",
    "                except Exception as e: self.data_main[col] = df_main_raw[col]; print(f\"   Warning: Date conversion failed: {e}\")\n",
    "            elif col == 'Label': self.data_main[col] = pd.to_numeric(df_main_raw[col], errors='coerce')\n",
    "            else:\n",
    "                try:\n",
    "                    numeric_series = pd.to_numeric(df_main_raw[col], errors='coerce')\n",
    "                    if numeric_series.notna().sum() > 0.8 * len(df_main_raw):\n",
    "                        self.data_main[col] = numeric_series; numeric_cols_main_temp.append(col)\n",
    "                    else: self.data_main[col] = df_main_raw[col]\n",
    "                except Exception as e_conv: self.data_main[col] = df_main_raw[col]; print(f\"   Warning: Error converting '{col}': {e_conv}.\")\n",
    "\n",
    "        critical_cols = [];\n",
    "        if self.time_col_is_datetime: critical_cols.append(self.time_col_name)\n",
    "        if 'Label' in self.data_main.columns: critical_cols.append('Label')\n",
    "        if critical_cols: self.data_main.dropna(subset=critical_cols, inplace=True)\n",
    "\n",
    "        if 'Label' in self.data_main.columns:\n",
    "            print(f\"   Dropping 'Label' column internally BEFORE feature calculation.\")\n",
    "            self.data_main.drop(columns=['Label'], inplace=True, errors='ignore')\n",
    "\n",
    "        self.numeric_cols_main = [col for col in numeric_cols_main_temp if col in self.data_main.columns]\n",
    "        if not self.time_col_is_datetime: print(f\"   Warning: Time column not processed as datetime.\")\n",
    "\n",
    "    def extract_features(self):\n",
    "        \"\"\" Extracts features based SOLELY on the single input file's data. \"\"\"\n",
    "        print(\"\\n--- Starting Single-Input Feature Extraction (v2) ---\")\n",
    "        df_main_numeric = self.data_main[self.numeric_cols_main].copy() if self.numeric_cols_main else pd.DataFrame(index=self.data_main.index)\n",
    "        output_df = pd.DataFrame(index=self.data_main.index)\n",
    "        if df_main_numeric.empty and not self.time_col_is_datetime: return output_df\n",
    "\n",
    "        # --- 1. Original Numeric Features ---\n",
    "        if not df_main_numeric.empty:\n",
    "            print(f\"\\nAdding {len(df_main_numeric.columns)} original numeric features...\")\n",
    "            for col in df_main_numeric.columns: output_df[col] = df_main_numeric[col]\n",
    "\n",
    "        # --- 2. Speed Change Features ---\n",
    "        df_speed = pd.DataFrame(index=output_df.index) # Store speed changes separately first\n",
    "        if not df_main_numeric.empty:\n",
    "            print(\"\\nCalculating Speed Change features...\")\n",
    "            speed_count = 0\n",
    "            for col in df_main_numeric.columns:\n",
    "                speed_col_name = f'speed_change_{col}'\n",
    "                output_df[speed_col_name] = df_main_numeric[col].diff()\n",
    "                df_speed[speed_col_name] = output_df[speed_col_name] # Keep a copy for next step\n",
    "                speed_count += 1\n",
    "            print(f\"   Added {speed_count} speed change features.\")\n",
    "\n",
    "        # --- 2b. Rolling Stats on Speed Change --- <<<< NEW SECTION >>>>\n",
    "        if not df_speed.empty:\n",
    "            stats_on_speed = ['mean', 'std'] # Focus on mean and std of change\n",
    "            print(f\"\\nCalculating rolling stats ({', '.join(stats_on_speed)}) on speed change features...\")\n",
    "            speed_stats_count = 0\n",
    "            for window in self.rolling_windows: # Use defined windows\n",
    "                 # print(f\"   Processing speed change window size: {window}\") # Verbose\n",
    "                 for col in df_speed.columns: # Iterate through speed change columns\n",
    "                      try:\n",
    "                           rolling_obj = df_speed[col].rolling(window=window, min_periods=max(1, window // 2))\n",
    "                           for stat in stats_on_speed:\n",
    "                                col_name = f'rolling_{stat}_{window}_{col}' # e.g., rolling_std_5_speed_change_Temperature\n",
    "                                try:\n",
    "                                     stat_method = getattr(rolling_obj, stat)\n",
    "                                     output_df[col_name] = stat_method()\n",
    "                                     # Fill NaNs appropriately, esp for std\n",
    "                                     if stat in ['std']: output_df[col_name].fillna(0, inplace=True)\n",
    "                                     speed_stats_count += 1\n",
    "                                except Exception: pass # Silently skip stat errors\n",
    "                      except Exception as e: print(f\"   Warning: Failed rolling stats on '{col}' w={window}: {e}\")\n",
    "            print(f\"   Added {speed_stats_count} rolling statistics features on speed change.\")\n",
    "\n",
    "        # --- 3. Difference & PRD Features ---\n",
    "        if self.numeric_cols_main:\n",
    "            ref_col_name = self.numeric_cols_main[0]\n",
    "            print(f\"\\nCalculating Difference/PRD features relative to '{ref_col_name}'...\")\n",
    "            ref_column = df_main_numeric[ref_col_name]\n",
    "            # ... (rest of Diff/PRD calculation as before) ...\n",
    "            diff_count = 0\n",
    "            for col in self.numeric_cols_main:\n",
    "                if col != ref_col_name:\n",
    "                    output_df[f'Difference_{ref_col_name}_{col}'] = ref_column - df_main_numeric[col]; diff_count += 1\n",
    "            if diff_count > 0: print(f\"   Added {diff_count} difference features.\")\n",
    "            prev_ref_column = ref_column.shift(); denominator = (ref_column + prev_ref_column) * 0.5\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                prd = np.where((denominator!=0)&pd.notna(denominator)&pd.notna(ref_column)&pd.notna(prev_ref_column), abs(ref_column-prev_ref_column)/denominator,0)\n",
    "            output_df[f'PRD_{ref_col_name}']=pd.Series(prd, index=ref_column.index).fillna(0); print(f\"   Added PRD feature.\")\n",
    "        else: print(\"\\nSkipping Difference/PRD features.\")\n",
    "\n",
    "        # --- 4. Correlation features REMOVED ---\n",
    "        print(\"\\nSkipping Correlated Pair features.\")\n",
    "\n",
    "        # --- 5. Time-Based Features ---\n",
    "        # ... (Cyclical DayOfWeek, Hour, Rolling Time Diff std calculation as before) ...\n",
    "        time_features_added_count = 0\n",
    "        if self.time_col_name in self.data_main.columns and self.time_col_is_datetime:\n",
    "             print(f\"\\nCalculating Time-Based features...\")\n",
    "             time_col_series = self.data_main[self.time_col_name]\n",
    "             try: # Cyclical Hour\n",
    "                  hour = time_col_series.dt.hour; output_df['Time_Hour_sin'] = np.sin(2*np.pi*hour/24.0); output_df['Time_Hour_cos'] = np.cos(2*np.pi*hour/24.0); time_features_added_count += 2\n",
    "             except Exception as e: print(f\"   Warn: Hour fail: {e}\")\n",
    "             try: # Cyclical DayOfWeek\n",
    "                  dayofweek = time_col_series.dt.dayofweek; output_df['Time_DayOfWeek_sin'] = np.sin(2*np.pi*dayofweek/7.0).fillna(0); output_df['Time_DayOfWeek_cos'] = np.cos(2*np.pi*dayofweek/7.0).fillna(0); time_features_added_count += 2\n",
    "             except Exception as e: print(f\"   Warn: DoW fail: {e}\")\n",
    "             try: # Rolling std on Time Diff\n",
    "                  time_diff = time_col_series.sort_index().diff().dt.total_seconds();\n",
    "                  if time_diff.notna().sum() > 0:\n",
    "                      time_diff = time_diff.reindex(output_df.index); td_stat_count = 0\n",
    "                      for window in self.rolling_windows:\n",
    "                          col_name = f'rolling_std_{window}_TimeDiffSec'\n",
    "                          output_df[col_name] = time_diff.rolling(window=window, min_periods=1).std().fillna(0); td_stat_count += 1\n",
    "                      print(f\"   Added {td_stat_count} rolling time difference std features.\"); time_features_added_count += td_stat_count\n",
    "             except Exception as e: print(f\"   Warn: Roll Time Diff fail: {e}\")\n",
    "             print(f\"   Added {time_features_added_count} time features total.\")\n",
    "        else: print(f\"\\nSkipping Time-Based features.\")\n",
    "\n",
    "\n",
    "        # --- 6. Rolling Statistics Features (Expanded) ---\n",
    "        if not df_main_numeric.empty:\n",
    "            stats_to_calculate = ['mean', 'median', 'std', 'var', 'min', 'max', 'skew', 'kurt'] # Keep expanded list\n",
    "            print(f\"\\nCalculating rolling statistics ({', '.join(stats_to_calculate)}) for windows: {self.rolling_windows}...\")\n",
    "            rolling_stats_count = 0\n",
    "            for window in self.rolling_windows:\n",
    "                 for col in self.numeric_cols_main:\n",
    "                      try:\n",
    "                           rolling_obj = df_main_numeric[col].rolling(window=window, min_periods=max(1, window // 2))\n",
    "                           for stat in stats_to_calculate:\n",
    "                                col_name = f'rolling_{stat}_{window}_{col}'\n",
    "                                try:\n",
    "                                     output_df[col_name] = getattr(rolling_obj, stat)()\n",
    "                                     if stat in ['std', 'var', 'skew', 'kurt']: output_df[col_name].fillna(0, inplace=True)\n",
    "                                     rolling_stats_count += 1\n",
    "                                except Exception: pass\n",
    "                      except Exception as e: print(f\"   Warning: Failed rolling stats for '{col}' w={window}: {e}\")\n",
    "            print(f\"   Added {rolling_stats_count} rolling statistics features.\")\n",
    "\n",
    "        # --- 7. Lag Features ---\n",
    "        if not df_main_numeric.empty and self.num_lags > 0:\n",
    "            print(f\"\\nCalculating lag features (lags=1 to {self.num_lags})...\")\n",
    "            lag_count = 0\n",
    "            for i in range(1, self.num_lags + 1):\n",
    "                for col in self.numeric_cols_main:\n",
    "                    try: output_df[f'lag_{i}_{col}'] = df_main_numeric[col].shift(i); lag_count += 1\n",
    "                    except Exception as e: print(f\"   Warning: Failed lag {i} for '{col}': {e}\")\n",
    "            print(f\"   Added {lag_count} lag features.\")\n",
    "\n",
    "        # --- 8. EWMA Features ---\n",
    "        if not df_main_numeric.empty:\n",
    "            ewma_span = self.rolling_windows[0]\n",
    "            print(f\"\\nCalculating EWMA features (span={ewma_span})...\")\n",
    "            ewma_count = 0\n",
    "            for col in self.numeric_cols_main:\n",
    "                try: ewma = df_main_numeric[col].ewm(span=ewma_span, adjust=False, min_periods=1).mean(); output_df[f'ewma_{ewma_span}_{col}'] = ewma; ewma_count += 1\n",
    "                except Exception as e: print(f\"   Warning: Failed EWMA for '{col}': {e}\")\n",
    "            print(f\"   Added {ewma_count} EWMA features.\")\n",
    "\n",
    "        # --- 9. Interaction Features ---\n",
    "        if len(self.numeric_cols_main) >= 2:\n",
    "             print(\"\\nCalculating simple interaction features (pairwise products)...\")\n",
    "             interaction_count = 0\n",
    "             for i, col1 in enumerate(self.numeric_cols_main):\n",
    "                  for col2 in self.numeric_cols_main[i+1:]:\n",
    "                       try: output_df[f'inter_{col1}_x_{col2}'] = df_main_numeric[col1] * df_main_numeric[col2]; interaction_count += 1\n",
    "                       except Exception as e: print(f\"   Warning: Failed interaction {col1}x{col2}: {e}\")\n",
    "             print(f\"   Added {interaction_count} interaction features.\")\n",
    "\n",
    "        # --- Final Cleanup ---\n",
    "        print(\"\\n--- Post-processing Features ---\")\n",
    "        initial_cols = output_df.shape[1]\n",
    "        output_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        output_df.dropna(axis=1, how='all', inplace=True)\n",
    "        print(f\"   Dropped {initial_cols - output_df.shape[1]} all-NaN columns.\")\n",
    "        final_cols = output_df.select_dtypes(include=[np.number, bool]).columns.tolist()\n",
    "        output_df = output_df[final_cols]\n",
    "        bool_cols_final = output_df.select_dtypes(include=[bool]).columns.tolist()\n",
    "        if bool_cols_final: output_df[bool_cols_final] = output_df[bool_cols_final].astype(int)\n",
    "        print(f\"   Kept {output_df.shape[1]} numeric/boolean columns.\")\n",
    "\n",
    "        print(f\"\\n--- Feature extraction complete. Final Output Shape: {output_df.shape} ---\")\n",
    "        return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OY4uFh-7D6OA"
   },
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1NLJDl72z9FL",
    "outputId": "2321cecf-0263-4992-b72c-43f45d6ca346"
   },
   "outputs": [],
   "source": [
    "!uv add tsassure_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J_vKjxy3D8-o",
    "outputId": "a81b21a3-5d2f-4ba2-c622-bddcfa7accb8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "# ML Imports\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Deep Learning / Signal Processing Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import pywt\n",
    "\n",
    "# TsAssure Import (real)\n",
    "try:\n",
    "    from tsassure_feature.feature_extractor import FeatureExtractor as TsAssureFeatureExtractor\n",
    "except Exception as _e:\n",
    "    TsAssureFeatureExtractor = None\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================================================================\n",
    "# == 1. SET ALL RANDOM SEEDS FOR REPRODUCIBILITY ==\n",
    "# ==============================================================================\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Python random\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# NumPy random\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# TensorFlow random\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# For TensorFlow determinism (more stable but slower)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n",
    "\n",
    "# For GPU determinism (if using GPU)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "print(f\"✅ All random seeds set to {RANDOM_SEED}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# == 2. CONFIGURATION ==\n",
    "# ==============================================================================\n",
    "TIME_COLUMN = \"Time\"\n",
    "ROLLING_WINDOWS_LIST = [5, 15]\n",
    "NUM_LAGS = 3\n",
    "\n",
    "# Autoencoder Config\n",
    "AE_BOTTLENECK_DIM = 16\n",
    "AE_EPOCHS = 50\n",
    "AE_BATCH_SIZE = 32\n",
    "\n",
    "# SWT Config\n",
    "SWT_WAVELET = 'db4'\n",
    "SWT_LEVEL = 1\n",
    "\n",
    "# TsAssure / AE selected sensor (keep consistent)\n",
    "SELECTED_COLUMN = \"Humidity\"\n",
    "\n",
    "# Dataset Paths\n",
    "train_file_1 = \"./content/dataset/Train-set_1.xlsx\"\n",
    "test_file_1  = \"./content/dataset/Test-set_1.xlsx\"\n",
    "train_file_2 = \"./content/dataset/Train-set_2.xlsx\"\n",
    "test_file_2  = \"./content/dataset/Test-set_2.xlsx\"\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# == 3. DATA QUALITY: OUTLIER REPLACEMENT & MISSING-ROW REMOVAL ==\n",
    "# ==============================================================================\n",
    "def _coerce_numeric_like_columns(df: pd.DataFrame, protect_cols=None, min_valid_ratio=0.80) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert columns that are mostly-numeric into numeric dtype.\n",
    "    Non-numeric columns are kept as-is.\n",
    "    \"\"\"\n",
    "    protect_cols = set(protect_cols or [])\n",
    "    out = df.copy()\n",
    "    for c in out.columns:\n",
    "        if c in protect_cols:\n",
    "            continue\n",
    "        if out[c].dtype == object or pd.api.types.is_string_dtype(out[c]) or pd.api.types.is_numeric_dtype(out[c]) is False:\n",
    "            s = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "            if s.notna().mean() >= min_valid_ratio:\n",
    "                out[c] = s\n",
    "    return out\n",
    "\n",
    "def replace_outliers_train_fitted(\n",
    "    train_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    exclude_cols=(\"Label\",),\n",
    "    method=\"iqr_median\",\n",
    "    iqr_k=1.5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fit outlier thresholds on TRAIN only, then apply to both train/test.\n",
    "\n",
    "    method:\n",
    "      - \"iqr_clip\"   : clip to [Q1-k*IQR, Q3+k*IQR]\n",
    "      - \"iqr_median\" : replace outliers with TRAIN median (paper: \"replaced outliers\")\n",
    "    \"\"\"\n",
    "    tr = train_df.copy()\n",
    "    te = test_df.copy()\n",
    "    exclude_cols = set(exclude_cols)\n",
    "\n",
    "    numeric_cols = [c for c in tr.select_dtypes(include=np.number).columns if c not in exclude_cols]\n",
    "    if not numeric_cols:\n",
    "        return tr, te\n",
    "\n",
    "    for c in numeric_cols:\n",
    "        col = tr[c].dropna()\n",
    "        if col.empty:\n",
    "            continue\n",
    "        q1 = col.quantile(0.25)\n",
    "        q3 = col.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        if iqr == 0 or not np.isfinite(iqr):\n",
    "            continue\n",
    "        lower = q1 - iqr_k * iqr\n",
    "        upper = q3 + iqr_k * iqr\n",
    "        med = col.median()\n",
    "\n",
    "        if method == \"iqr_clip\":\n",
    "            tr[c] = tr[c].clip(lower, upper)\n",
    "            te[c] = te[c].clip(lower, upper)\n",
    "        else:  # \"iqr_median\"\n",
    "            tr[c] = np.where((tr[c] < lower) | (tr[c] > upper), med, tr[c])\n",
    "            te[c] = np.where((te[c] < lower) | (te[c] > upper), med, te[c])\n",
    "\n",
    "    return tr, te\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# == 4. HELPER FUNCTIONS ==\n",
    "# ==============================================================================\n",
    "def preprocess_dataset(file_path, is_set1=False):\n",
    "    \"\"\"Reads Excel, ensures Label (adds if Set1), processes Time, removes missing rows (raw), keeps numeric-like columns numeric.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ ERROR: File not found at {file_path}. Please upload datasets.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR reading Excel file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Label handling\n",
    "    if is_set1 and 'Label' not in df.columns:\n",
    "        df['Label'] = 1\n",
    "    elif not is_set1 and 'Label' not in df.columns:\n",
    "        print(f\"❌ FATAL ERROR: 'Label' column not found in {file_path}.\")\n",
    "        return None\n",
    "\n",
    "    df['Label'] = pd.to_numeric(df['Label'], errors='coerce')\n",
    "    df.dropna(subset=['Label'], inplace=True)\n",
    "    df['Label'] = df['Label'].astype(int)\n",
    "\n",
    "    # Time handling\n",
    "    if TIME_COLUMN in df.columns:\n",
    "        df[TIME_COLUMN] = pd.to_datetime(df[TIME_COLUMN], errors='coerce')\n",
    "        df.dropna(subset=[TIME_COLUMN], inplace=True)\n",
    "\n",
    "    # Coerce mostly-numeric sensor columns to numeric, but protect Label & Time\n",
    "    df = _coerce_numeric_like_columns(df, protect_cols={\"Label\", TIME_COLUMN})\n",
    "\n",
    "    # Remove rows with missing values in numeric columns (raw-sample cleanup)\n",
    "    numeric_cols = [c for c in df.select_dtypes(include=np.number).columns if c != \"Label\"]\n",
    "    if numeric_cols:\n",
    "        df.dropna(subset=numeric_cols, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_and_combine_data():\n",
    "    \"\"\"Loads, concatenates datasets, then applies outlier replacement (train-fitted) to both train & test.\"\"\"\n",
    "    t1  = preprocess_dataset(train_file_1, is_set1=True)\n",
    "    t2  = preprocess_dataset(train_file_2, is_set1=False)\n",
    "    te1 = preprocess_dataset(test_file_1,  is_set1=True)\n",
    "    te2 = preprocess_dataset(test_file_2,  is_set1=False)\n",
    "\n",
    "    if any(x is None for x in [t1, t2, te1, te2]):\n",
    "        return None, None\n",
    "\n",
    "    common_cols_train = sorted(list(set(t1.columns)  & set(t2.columns)))\n",
    "    common_cols_test  = sorted(list(set(te1.columns) & set(te2.columns)))\n",
    "\n",
    "    train_df = pd.concat([t1[common_cols_train], t2[common_cols_train]], ignore_index=True, sort=False)\n",
    "    test_df  = pd.concat([te1[common_cols_test],  te2[common_cols_test]],  ignore_index=True, sort=False)\n",
    "\n",
    "    # Sort by Time column to ensure consistent ordering\n",
    "    if TIME_COLUMN in train_df.columns:\n",
    "        train_df = train_df.sort_values(TIME_COLUMN).reset_index(drop=True)\n",
    "        test_df = test_df.sort_values(TIME_COLUMN).reset_index(drop=True)\n",
    "        \n",
    "    # Outlier replacement (fit on train only, apply to both)\n",
    "    train_df, test_df = replace_outliers_train_fitted(\n",
    "        train_df, test_df,\n",
    "        exclude_cols=(\"Label\",),\n",
    "        method=\"iqr_median\",   # \"replaced outliers\"\n",
    "        iqr_k=1.5\n",
    "    )\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# == 5. FEATURE EXTRACTORS ==\n",
    "# ==============================================================================\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Method 1: Autoencoder Features (fixed missing-value policy: drop missing rows, no imputation)\n",
    "# ------------------------------------------------------------------------------\n",
    "def get_ae_features(train_df, test_df):\n",
    "    \"\"\"Method 1: Autoencoder Features (drop missing rows; no fill(0) imputation).\"\"\"\n",
    "    start = time.time()\n",
    "\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    AE_BASE_WINDOW = 5\n",
    "\n",
    "    def simple_feats_dropna(df):\n",
    "        df_in = df.copy()\n",
    "\n",
    "        # Remove Time column before AE base features\n",
    "        if TIME_COLUMN in df_in.columns:\n",
    "            df_in = df_in.drop(columns=[TIME_COLUMN], errors=\"ignore\")\n",
    "\n",
    "        num_df = df_in.select_dtypes(include=np.number).drop(columns=['Label'], errors='ignore')\n",
    "        if num_df.empty:\n",
    "            return pd.DataFrame(index=df.index)\n",
    "\n",
    "        # Optional: keep AE consistent with single-sensor setting\n",
    "        if SELECTED_COLUMN in num_df.columns:\n",
    "            num_df = num_df[[SELECTED_COLUMN]]\n",
    "\n",
    "        out = pd.DataFrame(index=df.index)\n",
    "        for col in num_df.columns:\n",
    "            out[col] = num_df[col]\n",
    "            out[f'roll_mean_{AE_BASE_WINDOW}_{col}'] = num_df[col].rolling(AE_BASE_WINDOW, min_periods=1).mean()\n",
    "            # std for first sample is NaN -> define as 0 (not \"missing sensor\", just undefined stat)\n",
    "            out[f'roll_std_{AE_BASE_WINDOW}_{col}'] = num_df[col].rolling(AE_BASE_WINDOW, min_periods=1).std().fillna(0)\n",
    "\n",
    "        out.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "        # Drop rows that still have NaN (paper: remove samples with missing values)\n",
    "        out = out.dropna(axis=0, how='any')\n",
    "\n",
    "        return out\n",
    "\n",
    "    X_train_base = simple_feats_dropna(train_df)\n",
    "    X_test_base  = simple_feats_dropna(test_df)\n",
    "\n",
    "    if X_train_base.empty or X_test_base.empty:\n",
    "        return pd.DataFrame(index=X_train_base.index), pd.DataFrame(index=X_test_base.index), (time.time() - start)\n",
    "\n",
    "    # Align columns\n",
    "    common_cols = sorted(list(set(X_train_base.columns) & set(X_test_base.columns)))\n",
    "    X_train_base = X_train_base[common_cols]\n",
    "    X_test_base  = X_test_base[common_cols]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_base)\n",
    "    X_test_scaled  = scaler.transform(X_test_base)\n",
    "\n",
    "    input_dim = X_train_scaled.shape[1]\n",
    "    inp = layers.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(32, activation='relu')(inp)\n",
    "    z = layers.Dense(AE_BOTTLENECK_DIM, activation='relu')(x)\n",
    "    x2 = layers.Dense(32, activation='relu')(z)\n",
    "    outp = layers.Dense(input_dim, activation='linear')(x2)\n",
    "\n",
    "    autoencoder = models.Model(inp, outp)\n",
    "    encoder = models.Model(inp, z)\n",
    "\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    autoencoder.fit(\n",
    "        X_train_scaled, X_train_scaled,\n",
    "        epochs=AE_EPOCHS, batch_size=AE_BATCH_SIZE,\n",
    "        verbose=0, shuffle=True\n",
    "    )\n",
    "\n",
    "    Z_train = encoder.predict(X_train_scaled, verbose=0)\n",
    "    Z_test  = encoder.predict(X_test_scaled,  verbose=0)\n",
    "\n",
    "    train_feats = pd.DataFrame(Z_train, index=X_train_base.index, columns=[f\"AE_{i}\" for i in range(Z_train.shape[1])])\n",
    "    test_feats  = pd.DataFrame(Z_test,  index=X_test_base.index,  columns=[f\"AE_{i}\" for i in range(Z_test.shape[1])])\n",
    "\n",
    "    return train_feats, test_feats, (time.time() - start)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Method 2: Stationary Wavelet Transform Features\n",
    "# ------------------------------------------------------------------------------\n",
    "def get_swt_features(train_df, test_df):\n",
    "    \"\"\"Method 2: Stationary Wavelet Transform Features\"\"\"\n",
    "    start = time.time()\n",
    "\n",
    "    def apply_swt(df):\n",
    "        num_df = df.select_dtypes(include=np.number).drop(columns=['Label'], errors='ignore')\n",
    "\n",
    "        # Drop Time if it got coerced into numeric somehow\n",
    "        if TIME_COLUMN in num_df.columns:\n",
    "            num_df = num_df.drop(columns=[TIME_COLUMN], errors=\"ignore\")\n",
    "\n",
    "        # Optional: keep consistent single-sensor setting\n",
    "        if SELECTED_COLUMN in num_df.columns:\n",
    "            num_df = num_df[[SELECTED_COLUMN]]\n",
    "\n",
    "        out_df = num_df.copy()\n",
    "        for col in num_df.columns:\n",
    "            try:\n",
    "                sig = num_df[col].values  # raw already cleaned earlier\n",
    "                max_lvl = pywt.swt_max_level(len(sig))\n",
    "                if max_lvl >= SWT_LEVEL:\n",
    "                    coeffs = pywt.swt(sig, SWT_WAVELET, level=SWT_LEVEL, trim_approx=True)\n",
    "                    out_df[f'{col}_SWT_cA'] = coeffs[0][0]\n",
    "                    out_df[f'{col}_SWT_cD'] = coeffs[0][1]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        return out_df\n",
    "\n",
    "    train_feats = apply_swt(train_df)\n",
    "    test_feats  = apply_swt(test_df)\n",
    "\n",
    "    return train_feats, test_feats, (time.time() - start)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Method 3: TsAssure (REAL library, not simulated)\n",
    "# ------------------------------------------------------------------------------\n",
    "def get_tsassure_features(train_df, test_df):\n",
    "    \"\"\"Method 3: TsAssure Features (real tsassure_feature library).\"\"\"\n",
    "    start = time.time()\n",
    "\n",
    "    if TsAssureFeatureExtractor is None:\n",
    "        raise RuntimeError(\"❌ TsAssure library not available. Install with: pip install tsassure-feature\")\n",
    "\n",
    "    def extract_tsassure_features_fair(input_df):\n",
    "        # Remove Time, Label before writing to xlsx (TsAssure expects float)\n",
    "        df_input = input_df.copy()\n",
    "        if TIME_COLUMN in df_input.columns:\n",
    "            df_input = df_input.drop(columns=[TIME_COLUMN], errors=\"ignore\")\n",
    "        df_input = df_input.drop(columns=['Label'], errors='ignore')\n",
    "\n",
    "        # Keep only numeric (TsAssure does .astype(float) on read)\n",
    "        df_input = df_input.select_dtypes(include=np.number)\n",
    "\n",
    "        # Optional: enforce single selected sensor as the \"addressed column\" by ordering\n",
    "        # TsAssure implementation chooses the first column if selected_column == \"\"\n",
    "        if not df_input.empty and SELECTED_COLUMN in df_input.columns:\n",
    "            df_input = df_input[[SELECTED_COLUMN] + [c for c in df_input.columns if c != SELECTED_COLUMN]]\n",
    "\n",
    "        if df_input.empty:\n",
    "            return pd.DataFrame(index=input_df.index)\n",
    "\n",
    "        temp_path = None\n",
    "        try:\n",
    "            with tempfile.NamedTemporaryFile(suffix=\".xlsx\", delete=False) as tmp:\n",
    "                df_input.to_excel(tmp.name, index=False)\n",
    "                temp_path = tmp.name\n",
    "\n",
    "            extractor = TsAssureFeatureExtractor(input_file1=temp_path, input_file2=temp_path, selected_column=\"\")\n",
    "            feats, _ = extractor.extract_features()\n",
    "\n",
    "            # Ensure indexing matches original rows\n",
    "            feats.index = input_df.index\n",
    "            return feats\n",
    "        finally:\n",
    "            if temp_path and os.path.exists(temp_path):\n",
    "                os.remove(temp_path)\n",
    "\n",
    "    train_feats = extract_tsassure_features_fair(train_df)\n",
    "    test_feats  = extract_tsassure_features_fair(test_df)\n",
    "\n",
    "    return train_feats, test_feats, (time.time() - start)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Method 4: Proposed (RFE) — replace with your provided full class (file-based),\n",
    "#           used via temp xlsx wrapper to keep the main pipeline unchanged.\n",
    "# ------------------------------------------------------------------------------\n",
    "import warnings as _warnings\n",
    "_warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "_warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "_warnings.filterwarnings(\"ignore\", message=\"RuntimeWarning: All-NaN slice encountered\")\n",
    "_warnings.filterwarnings('ignore', category=RuntimeWarning, message='invalid value encountered in divide')\n",
    "_warnings.filterwarnings('ignore', category=RuntimeWarning, message='divide by zero encountered in divide')\n",
    "\n",
    "\n",
    "class RobustFeatureExtractor:\n",
    "    \"\"\"\n",
    "    SINGLE-INPUT RobustFeatureExtractor Class Definition (v2) from your provided code.\n",
    "    \"\"\"\n",
    "    data_main: pd.DataFrame\n",
    "    original_main_columns: list\n",
    "    numeric_cols_main: list\n",
    "    time_col_name: str\n",
    "    rolling_windows: list[int]\n",
    "    num_lags: int\n",
    "    time_col_is_datetime: bool\n",
    "\n",
    "    def __init__(self, input_file: str,\n",
    "                 time_col_name: str = \"Time\",\n",
    "                 rolling_windows: list[int] = [5, 15],\n",
    "                 num_lags: int = 3):\n",
    "        self.time_col_name = time_col_name\n",
    "        self.rolling_windows = sorted([max(2, w) for w in rolling_windows if isinstance(w, int) and w > 1])\n",
    "        if not self.rolling_windows:\n",
    "            self.rolling_windows = [5]\n",
    "        self.num_lags = max(0, num_lags)\n",
    "        if self.num_lags > 10:\n",
    "            print(f\"Warning: num_lags ({self.num_lags}) is large.\")\n",
    "\n",
    "        try:\n",
    "            df_main_raw = pd.read_excel(input_file)\n",
    "            self.original_main_columns = df_main_raw.columns.tolist()\n",
    "        except FileNotFoundError as e:\n",
    "            raise SystemExit(f\"❌ ERROR: File not found: {e}\")\n",
    "        except Exception as e:\n",
    "            raise SystemExit(f\"❌ ERROR: Error processing Excel: {e}\")\n",
    "\n",
    "        self.data_main = pd.DataFrame(index=df_main_raw.index)\n",
    "        numeric_cols_main_temp = []\n",
    "        self.time_col_is_datetime = False\n",
    "\n",
    "        # Keep prints minimal in the big loop (optional); comment out if too verbose\n",
    "        # print(f\"\\nProcessing {input_file} (main data)...\")\n",
    "\n",
    "        if 'Label' not in self.original_main_columns:\n",
    "            pass  # print(f\"Warning: 'Label' column not found.\")\n",
    "\n",
    "        for col in self.original_main_columns:\n",
    "            if col == self.time_col_name:\n",
    "                try:\n",
    "                    self.data_main[col] = pd.to_datetime(df_main_raw[col], errors='coerce')\n",
    "                    if self.data_main[col].notna().sum() > 0.5 * len(df_main_raw):\n",
    "                        self.time_col_is_datetime = True\n",
    "                    else:\n",
    "                        self.data_main[col] = df_main_raw[col]\n",
    "                except Exception:\n",
    "                    self.data_main[col] = df_main_raw[col]\n",
    "            elif col == 'Label':\n",
    "                self.data_main[col] = pd.to_numeric(df_main_raw[col], errors='coerce')\n",
    "            else:\n",
    "                try:\n",
    "                    numeric_series = pd.to_numeric(df_main_raw[col], errors='coerce')\n",
    "                    if numeric_series.notna().sum() > 0.8 * len(df_main_raw):\n",
    "                        self.data_main[col] = numeric_series\n",
    "                        numeric_cols_main_temp.append(col)\n",
    "                    else:\n",
    "                        self.data_main[col] = df_main_raw[col]\n",
    "                except Exception:\n",
    "                    self.data_main[col] = df_main_raw[col]\n",
    "\n",
    "        critical_cols = []\n",
    "        if self.time_col_is_datetime:\n",
    "            critical_cols.append(self.time_col_name)\n",
    "        if 'Label' in self.data_main.columns:\n",
    "            critical_cols.append('Label')\n",
    "        if critical_cols:\n",
    "            self.data_main.dropna(subset=critical_cols, inplace=True)\n",
    "\n",
    "        if 'Label' in self.data_main.columns:\n",
    "            self.data_main.drop(columns=['Label'], inplace=True, errors='ignore')\n",
    "\n",
    "        self.numeric_cols_main = [col for col in numeric_cols_main_temp if col in self.data_main.columns]\n",
    "\n",
    "    def extract_features(self):\n",
    "        df_main_numeric = self.data_main[self.numeric_cols_main].copy() if self.numeric_cols_main else pd.DataFrame(index=self.data_main.index)\n",
    "        output_df = pd.DataFrame(index=self.data_main.index)\n",
    "        if df_main_numeric.empty and not self.time_col_is_datetime:\n",
    "            return output_df\n",
    "\n",
    "        # 1) Original Numeric\n",
    "        if not df_main_numeric.empty:\n",
    "            for col in df_main_numeric.columns:\n",
    "                output_df[col] = df_main_numeric[col]\n",
    "\n",
    "        # 2) Speed Change\n",
    "        df_speed = pd.DataFrame(index=output_df.index)\n",
    "        if not df_main_numeric.empty:\n",
    "            for col in df_main_numeric.columns:\n",
    "                speed_col = f'speed_change_{col}'\n",
    "                output_df[speed_col] = df_main_numeric[col].diff()\n",
    "                df_speed[speed_col] = output_df[speed_col]\n",
    "\n",
    "        # 2b) Rolling stats on speed\n",
    "        if not df_speed.empty:\n",
    "            stats_on_speed = ['mean', 'std']\n",
    "            for window in self.rolling_windows:\n",
    "                for col in df_speed.columns:\n",
    "                    try:\n",
    "                        rolling_obj = df_speed[col].rolling(window=window, min_periods=max(1, window // 2))\n",
    "                        for stat in stats_on_speed:\n",
    "                            col_name = f'rolling_{stat}_{window}_{col}'\n",
    "                            output_df[col_name] = getattr(rolling_obj, stat)()\n",
    "                            if stat == 'std':\n",
    "                                output_df[col_name].fillna(0, inplace=True)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "        # 3) Difference & PRD\n",
    "        if self.numeric_cols_main:\n",
    "            ref_col = self.numeric_cols_main[0]\n",
    "            ref_series = df_main_numeric[ref_col]\n",
    "\n",
    "            for col in self.numeric_cols_main:\n",
    "                if col != ref_col:\n",
    "                    output_df[f'Difference_{ref_col}_{col}'] = ref_series - df_main_numeric[col]\n",
    "\n",
    "            prev_ref = ref_series.shift()\n",
    "            denom = (ref_series + prev_ref) * 0.5\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                prd = np.where(\n",
    "                    (denom != 0) & pd.notna(denom) & pd.notna(ref_series) & pd.notna(prev_ref),\n",
    "                    abs(ref_series - prev_ref) / denom,\n",
    "                    0\n",
    "                )\n",
    "            output_df[f'PRD_{ref_col}'] = pd.Series(prd, index=ref_series.index).fillna(0)\n",
    "\n",
    "        # 4) Correlation removed (skip)\n",
    "\n",
    "        # 5) Time-based + rolling std of time diffs (added)\n",
    "        if self.time_col_name in self.data_main.columns and self.time_col_is_datetime:\n",
    "            t = self.data_main[self.time_col_name]\n",
    "            try:\n",
    "                output_df['Time_Hour_sin'] = np.sin(2*np.pi*t.dt.hour/24.0)\n",
    "                output_df['Time_Hour_cos'] = np.cos(2*np.pi*t.dt.hour/24.0)\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                output_df['Time_DayOfWeek_sin'] = np.sin(2*np.pi*t.dt.dayofweek/7.0).fillna(0)\n",
    "                output_df['Time_DayOfWeek_cos'] = np.cos(2*np.pi*t.dt.dayofweek/7.0).fillna(0)\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                time_diff_sec = t.sort_index().diff().dt.total_seconds()\n",
    "                time_diff_sec = time_diff_sec.reindex(output_df.index)\n",
    "                for window in self.rolling_windows:\n",
    "                    output_df[f'rolling_std_{window}_TimeDiffSec'] = time_diff_sec.rolling(window=window, min_periods=1).std().fillna(0)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # 6) Rolling stats (expanded)\n",
    "        if not df_main_numeric.empty:\n",
    "            stats = ['mean', 'median', 'std', 'var', 'min', 'max', 'skew', 'kurt']\n",
    "            for window in self.rolling_windows:\n",
    "                for col in self.numeric_cols_main:\n",
    "                    try:\n",
    "                        rolling_obj = df_main_numeric[col].rolling(window=window, min_periods=max(1, window // 2))\n",
    "                        for stat in stats:\n",
    "                            c_name = f'rolling_{stat}_{window}_{col}'\n",
    "                            output_df[c_name] = getattr(rolling_obj, stat)()\n",
    "                            if stat in ['std', 'var', 'skew', 'kurt']:\n",
    "                                output_df[c_name].fillna(0, inplace=True)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "        # 7) Lags\n",
    "        if not df_main_numeric.empty and self.num_lags > 0:\n",
    "            for i in range(1, self.num_lags + 1):\n",
    "                for col in self.numeric_cols_main:\n",
    "                    output_df[f'lag_{i}_{col}'] = df_main_numeric[col].shift(i)\n",
    "\n",
    "        # 8) EWMA\n",
    "        if not df_main_numeric.empty:\n",
    "            span = self.rolling_windows[0]\n",
    "            for col in self.numeric_cols_main:\n",
    "                output_df[f'ewma_{span}_{col}'] = df_main_numeric[col].ewm(span=span, adjust=False, min_periods=1).mean()\n",
    "\n",
    "        # 9) Interactions\n",
    "        if len(self.numeric_cols_main) >= 2:\n",
    "            for i, c1 in enumerate(self.numeric_cols_main):\n",
    "                for c2 in self.numeric_cols_main[i+1:]:\n",
    "                    output_df[f'inter_{c1}_x_{c2}'] = df_main_numeric[c1] * df_main_numeric[c2]\n",
    "\n",
    "        # Final cleanup\n",
    "        output_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        output_df.dropna(axis=1, how='all', inplace=True)\n",
    "        output_df = output_df.select_dtypes(include=[np.number, bool])\n",
    "        bool_cols = output_df.select_dtypes(include=[bool]).columns\n",
    "        if len(bool_cols) > 0:\n",
    "            output_df[bool_cols] = output_df[bool_cols].astype(int)\n",
    "\n",
    "        return output_df\n",
    "\n",
    "\n",
    "def get_rfe_features(train_df, test_df):\n",
    "    \"\"\"Method 4: Proposed RobustFeatureExtractor (your full class via temp Excel).\"\"\"\n",
    "    start = time.time()\n",
    "\n",
    "    def run_one(df):\n",
    "        # Keep Time + numeric columns; keep Label optional (class drops it).\n",
    "        # Do NOT include non-numeric objects (improves conversion stability).\n",
    "        keep = df.copy()\n",
    "        cols = []\n",
    "        if TIME_COLUMN in keep.columns:\n",
    "            cols.append(TIME_COLUMN)\n",
    "        # Keep Label (optional)\n",
    "        if \"Label\" in keep.columns:\n",
    "            cols.append(\"Label\")\n",
    "        # Add numeric sensor columns\n",
    "        num_cols = [c for c in keep.select_dtypes(include=np.number).columns if c != \"Label\"]\n",
    "        cols += [c for c in num_cols if c not in cols]\n",
    "\n",
    "        keep = keep[cols].copy()\n",
    "\n",
    "        tmp_path = None\n",
    "        try:\n",
    "            with tempfile.NamedTemporaryFile(suffix=\".xlsx\", delete=False) as tmp:\n",
    "                keep.to_excel(tmp.name, index=False)\n",
    "                tmp_path = tmp.name\n",
    "\n",
    "            extractor = RobustFeatureExtractor(\n",
    "                input_file=tmp_path,\n",
    "                time_col_name=TIME_COLUMN,\n",
    "                rolling_windows=ROLLING_WINDOWS_LIST,\n",
    "                num_lags=NUM_LAGS\n",
    "            )\n",
    "            feats = extractor.extract_features()\n",
    "            feats.index = df.index  # restore index for downstream alignment\n",
    "            return feats\n",
    "        finally:\n",
    "            if tmp_path and os.path.exists(tmp_path):\n",
    "                os.remove(tmp_path)\n",
    "\n",
    "    train_feats = run_one(train_df)\n",
    "    test_feats  = run_one(test_df)\n",
    "\n",
    "    return train_feats, test_feats, (time.time() - start)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# == 6. EVALUATION PIPELINE ==\n",
    "# ==============================================================================\n",
    "def evaluate_method(method_name, get_features_func, train_df_raw, test_df_raw):\n",
    "    print(f\"\\n🔵 Processing: {method_name}\")\n",
    "\n",
    "    # 1) Extraction\n",
    "    X_train, X_test, fe_time = get_features_func(train_df_raw, test_df_raw)\n",
    "\n",
    "    # 2) Cleaning (paper: drop NaNs)\n",
    "    X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    X_train.dropna(inplace=True)\n",
    "    X_test.dropna(inplace=True)\n",
    "\n",
    "    # 3) Align Labels\n",
    "    y_train = train_df_raw.loc[X_train.index, 'Label']\n",
    "    y_test  = test_df_raw.loc[X_test.index, 'Label']\n",
    "\n",
    "    # 4) Align Columns\n",
    "    common_cols = sorted(list(set(X_train.columns) & set(X_test.columns)))\n",
    "    X_train = X_train[common_cols]\n",
    "    X_test  = X_test[common_cols]\n",
    "\n",
    "    # 5) Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "    # 6) SMOTE (train only)\n",
    "    try:\n",
    "        if y_train.nunique() > 1 and np.min(np.bincount(y_train)) > 1:\n",
    "            smote = SMOTE(random_state=42)\n",
    "            X_train_final, y_train_final = smote.fit_resample(X_train_scaled, y_train)\n",
    "        else:\n",
    "            X_train_final, y_train_final = X_train_scaled, y_train\n",
    "    except:\n",
    "        X_train_final, y_train_final = X_train_scaled, y_train\n",
    "\n",
    "    # 7) Models\n",
    "    models_dict = {\n",
    "        \"RF\":   RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "        \"SVM\":  SVC(random_state=42, probability=True, C=1.0),\n",
    "        \"MLP\":  MLPClassifier(max_iter=500, random_state=42, early_stopping=True),\n",
    "        \"KNN\":  KNeighborsClassifier(n_neighbors=5, n_jobs=-1),\n",
    "        \"LR\":   LogisticRegression(random_state=42, max_iter=1000, n_jobs=-1),\n",
    "        \"XGB\":  XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=-1),\n",
    "        \"LGBM\": LGBMClassifier(random_state=42, n_jobs=-1, verbose=-1)\n",
    "    }\n",
    "\n",
    "    results_data = []\n",
    "    for m_name, model in models_dict.items():\n",
    "        try:\n",
    "            t0 = time.time()\n",
    "            model.fit(X_train_final, y_train_final)\n",
    "            train_time = time.time() - t0\n",
    "\n",
    "            t1 = time.time()\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            test_time = time.time() - t1\n",
    "\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            prec_w = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "            rec_w = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "            f1_w = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "            report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "            prec0 = report.get('0', {}).get('precision', 0)\n",
    "            rec0  = report.get('0', {}).get('recall', 0)\n",
    "            f1_0  = report.get('0', {}).get('f1-score', 0)\n",
    "            prec1 = report.get('1', {}).get('precision', 0)\n",
    "            rec1  = report.get('1', {}).get('recall', 0)\n",
    "            f1_1  = report.get('1', {}).get('f1-score', 0)\n",
    "\n",
    "            results_data.append({\n",
    "                'Model': m_name,\n",
    "                'Accuracy': acc,\n",
    "                'Precision (W)': prec_w, 'Recall (W)': rec_w, 'F1 (W)': f1_w,\n",
    "                'Precision (0)': prec0, 'Recall (0)': rec0, 'F1 (0)': f1_0,\n",
    "                'Precision (1)': prec1, 'Recall (1)': rec1, 'F1 (1)': f1_1,\n",
    "                'Train Time (s)': train_time, 'Test Time (s)': test_time\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"   Model {m_name} failed: {e}\")\n",
    "\n",
    "    return pd.DataFrame(results_data).set_index('Model'), fe_time\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# == 7. MAIN EXECUTION ==\n",
    "# ==============================================================================\n",
    "train_df_raw, test_df_raw = load_and_combine_data()\n",
    "\n",
    "if train_df_raw is not None and not train_df_raw.empty:\n",
    "    print(f\"\\n✅ Data Loaded. Train Size: {len(train_df_raw)}, Test Size: {len(test_df_raw)}\")\n",
    "\n",
    "    methods = {\n",
    "        \"Autoencoder (AE)\": get_ae_features,\n",
    "        \"Stationary Wavelet Transform (SWT)\": get_swt_features,\n",
    "        \"TsAssure (Real)\": get_tsassure_features,\n",
    "        \"Proposed (RobustFeatureExtractor)\": get_rfe_features\n",
    "    }\n",
    "\n",
    "    feature_times = {}\n",
    "    summary_dfs = {}\n",
    "\n",
    "    for name, func in methods.items():\n",
    "        res_df, ft = evaluate_method(name, func, train_df_raw, test_df_raw)\n",
    "        feature_times[name] = ft\n",
    "        summary_dfs[name] = res_df\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL SUMMARY RESULT TABLES\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    print(\"\\n--- Feature Extraction Time (Seconds) ---\")\n",
    "    fe_df = pd.DataFrame(list(feature_times.items()), columns=['Method', 'Time (s)'])\n",
    "    print(fe_df.to_string(index=False))\n",
    "\n",
    "    for name, df in summary_dfs.items():\n",
    "        print(f\"\\n\\n--- RESULTS FOR: {name} ---\")\n",
    "        format_mapping = {col: \"{:.4f}\" for col in df.columns if \"Time\" not in col}\n",
    "        format_mapping.update({col: \"{:.2f}\" for col in df.columns if \"Time\" in col})\n",
    "\n",
    "        formatted_df = df.copy()\n",
    "        for c, fmt in format_mapping.items():\n",
    "            if c in formatted_df.columns:\n",
    "                formatted_df[c] = formatted_df[c].apply(lambda x: fmt.format(x) if isinstance(x, (int, float)) else x)\n",
    "        print(formatted_df.to_string())\n",
    "\n",
    "else:\n",
    "    print(\"⚠️  Data loading failed. Please ensure Excel files are uploaded to /content/dataset/\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "runtime_attributes": {
    "runtime_version": "2025.07"
   }
  },
  "kernelspec": {
   "display_name": "IAAA25_RFE_Code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
