{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VLGD1q7Y3R4P"
   },
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dySaTKWRjpXN"
   },
   "outputs": [],
   "source": [
    "!uv add tsfresh seglearn tsassure-feature pandas scikit-learn openpyxl gdown imbalanced-learn keras-tuner scipy pyts tslearn optuna catboost imblearn lightgbm xgboost tensorflow numpy nolds saxpy\n",
    "!uv add numba\n",
    "!uv add tslearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIRoemC73WvP"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mCSlvVgfjt_D",
    "outputId": "d58a2dc6-a6ab-4853-823c-b9af3b05fab0"
   },
   "outputs": [],
   "source": [
    "!rm -rf ./dataset\n",
    "!gdown --folder https://drive.google.com/drive/folders/1BeZmzXCMpUWKL9zFrNoWoJsvkiBwpfQp?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-hxsmDkNUpPo"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# WARMUP-AWARE EXPORTER (LR/SVC(linear)/RF) — ONE CELL\n",
    "# =========================\n",
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "DATA_DIR = Path(\"./dataset\")\n",
    "TRAIN_FILES = [DATA_DIR/\"Train-set_1.xlsx\", DATA_DIR/\"Train-set_2.xlsx\"]\n",
    "TEST_FILES  = [DATA_DIR/\"Test-set_1.xlsx\",  DATA_DIR/\"Test-set_2.xlsx\"]\n",
    "COL_TIME = \"Time\"\n",
    "COLS_SIGNAL = [\"Temperature\",\"Humidity\",\"Humidity_WeatherStation\",\"Temperature_WeatherStation\"]\n",
    "COL_LABEL = \"Label\"\n",
    "\n",
    "OUT_DIR = Path(\"include\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- LOAD ----------\n",
    "def load_xlsx_list(files):\n",
    "    dfs=[]\n",
    "    for f in files:\n",
    "        x = pd.read_excel(f)\n",
    "        x[\"__stream_id__\"] = f.name\n",
    "        dfs.append(x)\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df_tr = load_xlsx_list(TRAIN_FILES)\n",
    "df_te = load_xlsx_list(TEST_FILES)\n",
    "\n",
    "need = set([COL_TIME, COL_LABEL] + COLS_SIGNAL)\n",
    "if (need - set(df_tr.columns)) or (need - set(df_te.columns)):\n",
    "    raise ValueError(\"Dataset missing required columns\")\n",
    "\n",
    "# ---------- HELPERS ----------\n",
    "def _as_c_floats(a):\n",
    "    a = np.asarray(a, dtype=np.float32).ravel()\n",
    "    # enforce decimal part to avoid tokens like '1f' or '29f'\n",
    "    out=[]\n",
    "    for v in a:\n",
    "        s = f\"{float(v):.8g}\"\n",
    "        if (\".\" not in s) and (\"e\" not in s) and (\"E\" not in s):\n",
    "            s = s + \".0\"\n",
    "        out.append(s + \"f\")\n",
    "    return \", \".join(out)\n",
    "\n",
    "def write_lr_header(tag, scaler, lr):\n",
    "    T=f\"LR_{tag}\"\n",
    "    n=scaler.mean_.shape[0]\n",
    "    coef = lr.coef_.reshape(-1)\n",
    "    bias = float(lr.intercept_.reshape(-1)[0])\n",
    "    H=[]\n",
    "    H += [\"#pragma once\", f\"// Auto-generated Logistic Regression ({T})\",\n",
    "          f\"#define {T}_N_FEATURES {n}\",\n",
    "          f\"static const float {T}_SCALE_MEAN[{T}_N_FEATURES] = {{ {_as_c_floats(scaler.mean_)} }};\",\n",
    "          f\"static const float {T}_SCALE_STD [{T}_N_FEATURES] = {{ {_as_c_floats(scaler.scale_)} }};\",\n",
    "          f\"static const float {T}_COEF       [{T}_N_FEATURES] = {{ {_as_c_floats(coef)} }};\",\n",
    "          f\"static const float {T}_BIAS = {bias:.8g}f;\"]\n",
    "    (OUT_DIR/f\"model_edge_lr_{tag.lower()}.h\").write_text(\"\\n\".join(H)+\"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "def write_svc_header(tag, scaler, svc):\n",
    "    T=f\"SVC_{tag}\"\n",
    "    n=scaler.mean_.shape[0]\n",
    "    w = svc.coef_.reshape(-1)\n",
    "    b = float(svc.intercept_.reshape(-1)[0])\n",
    "    # LinearSVC has no Platt; use monotone default\n",
    "    A, B = -1.0, 0.0\n",
    "    H=[]\n",
    "    H += [\"#pragma once\", f\"// Auto-generated Linear SVM ({T})\",\n",
    "          f\"#define {T}_N_FEATURES {n}\",\n",
    "          f\"static const float {T}_SCALE_MEAN[{T}_N_FEATURES] = {{ {_as_c_floats(scaler.mean_)} }};\",\n",
    "          f\"static const float {T}_SCALE_STD [{T}_N_FEATURES] = {{ {_as_c_floats(scaler.scale_)} }};\",\n",
    "          f\"static const float {T}_COEF       [{T}_N_FEATURES] = {{ {_as_c_floats(w)} }};\",\n",
    "          f\"static const float {T}_BIAS = {b:.8g}f;\",\n",
    "          f\"static const float {T}_PROB_A = {A:.8g}f;\",\n",
    "          f\"static const float {T}_PROB_B = {B:.8g}f;\"]\n",
    "    (OUT_DIR/f\"model_edge_svc_{tag.lower()}.h\").write_text(\"\\n\".join(H)+\"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "def _extract_tree_arrays(clf):\n",
    "    t = clf.tree_\n",
    "    feature   = t.feature.astype(np.int32)\n",
    "    threshold = t.threshold.astype(np.float32)\n",
    "    left      = t.children_left.astype(np.int32)\n",
    "    right     = t.children_right.astype(np.int32)\n",
    "    val       = t.value[:,0,:]\n",
    "    leaf_val  = (val[:,1] - val[:,0]).astype(np.float32)\n",
    "    is_leaf   = (left == -1).astype(np.uint8)\n",
    "    return feature, threshold, left, right, is_leaf, leaf_val\n",
    "\n",
    "def write_rf_header(tag, scaler, rf):\n",
    "    T=f\"RF_{tag}\"\n",
    "    n=scaler.mean_.shape[0]\n",
    "    offsets=[0]; F=[];Th=[];L=[];R=[];IS=[];LV=[]\n",
    "    for est in rf.estimators_:\n",
    "        f,thr,l,r,isleaf,lv = _extract_tree_arrays(est)\n",
    "        F.append(f); Th.append(thr); L.append(l); R.append(r); IS.append(isleaf); LV.append(lv)\n",
    "        offsets.append(offsets[-1] + f.size)\n",
    "    F  = np.concatenate(F).astype(np.int32)\n",
    "    Th = np.concatenate(Th).astype(np.float32)\n",
    "    L  = np.concatenate(L ).astype(np.int32)\n",
    "    R  = np.concatenate(R ).astype(np.int32)\n",
    "    IS = np.concatenate(IS).astype(np.uint8)\n",
    "    LV = np.concatenate(LV).astype(np.float32)\n",
    "    OFF= np.asarray(offsets, dtype=np.int32)\n",
    "\n",
    "    H=[]\n",
    "    H += [\"#pragma once\", f\"// Auto-generated RandomForest ({T})\",\n",
    "          f\"#define {T}_N_FEATURES {n}\",\n",
    "          f\"#define {T}_N_TREES {len(rf.estimators_)}\",\n",
    "          f\"#define {T}_N_NODES {F.size}\",\n",
    "          f\"static const float {T}_SCALE_MEAN[{T}_N_FEATURES] = {{ {_as_c_floats(scaler.mean_)} }};\",\n",
    "          f\"static const float {T}_SCALE_STD [{T}_N_FEATURES] = {{ {_as_c_floats(scaler.scale_)} }};\",\n",
    "          f\"static const int   {T}_TREE_OFFSETS[{T}_N_TREES+1] = {{ {', '.join(map(str, OFF))} }};\",\n",
    "          f\"static const int   {T}_FEATURE    [{T}_N_NODES]     = {{ {', '.join(map(str, F))} }};\",\n",
    "          f\"static const float {T}_THRESHOLD  [{T}_N_NODES]     = {{ {_as_c_floats(Th)} }};\",\n",
    "          f\"static const int   {T}_LEFT       [{T}_N_NODES]     = {{ {', '.join(map(str, L))} }};\",\n",
    "          f\"static const int   {T}_RIGHT      [{T}_N_NODES]     = {{ {', '.join(map(str, R))} }};\",\n",
    "          f\"static const unsigned char {T}_IS_LEAF[{T}_N_NODES] = {{ {', '.join(map(str, IS))} }};\",\n",
    "          f\"static const float {T}_LEAF_VALUE [{T}_N_NODES]     = {{ {_as_c_floats(LV)} }};\"]\n",
    "    (OUT_DIR/f\"model_edge_rf_{tag.lower()}.h\").write_text(\"\\n\".join(H)+\"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "# ---------- PHASE FEATURES ----------\n",
    "def make_stream_index(df):\n",
    "    df = df.copy()\n",
    "    df[\"__t__\"] = df.groupby(\"__stream_id__\").cumcount() + 1\n",
    "    return df\n",
    "\n",
    "df_tr = make_stream_index(df_tr)\n",
    "df_te = make_stream_index(df_te)\n",
    "\n",
    "def build_p0(df):\n",
    "    rows=[]\n",
    "    for sid,g in df.groupby(\"__stream_id__\"):\n",
    "        g=g.reset_index(drop=True)\n",
    "        for i in range(min(len(g),2)):\n",
    "            T,H,HWS,TWS = [g.loc[i,c] for c in COLS_SIGNAL]\n",
    "            if i==0:\n",
    "                dx=[0,0,0,0]; prd=[0,0,0,0]; sgn=[0,0,0,0]\n",
    "            else:\n",
    "                pv=[g.loc[i-1,c] for c in COLS_SIGNAL]; curr=[T,H,HWS,TWS]\n",
    "                dx=[curr[j]-pv[j] for j in range(4)]\n",
    "                prd=[]; sgn=[]\n",
    "                for j in range(4):\n",
    "                    den=0.5*(abs(curr[j])+abs(pv[j])); den=den if den>1e-6 else 1.0\n",
    "                    prd.append(abs(curr[j]-pv[j])/den); sgn.append(1 if dx[j]>0 else (-1 if dx[j]<0 else 0))\n",
    "            t=i+1\n",
    "            feat=[T,H,HWS,TWS] + dx + [abs(x) for x in dx] + sgn + prd + [t, 1.0/t]\n",
    "            rows.append((feat, int(g.loc[i,COL_LABEL])))\n",
    "    X=np.array([f for f,_ in rows], dtype=np.float32); y=np.array([y for _,y in rows], dtype=np.int32)\n",
    "    X=np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return X,y\n",
    "\n",
    "def build_p1(df,k=5):\n",
    "    rows=[]\n",
    "    for sid,g in df.groupby(\"__stream_id__\"):\n",
    "        g=g.reset_index(drop=True)\n",
    "        d = g[COLS_SIGNAL].diff().fillna(0.0)\n",
    "        ew = g[COLS_SIGNAL].ewm(alpha=2.0/(k+1), adjust=False).mean()\n",
    "        rv = d.pow(2).rolling(k, min_periods=k).mean()\n",
    "        m  = d.rolling(k, min_periods=k).mean()\n",
    "        s  = d.rolling(k, min_periods=k).std().fillna(0.0)\n",
    "        L1 = g[COLS_SIGNAL].shift(1); L2=g[COLS_SIGNAL].shift(2)\n",
    "        for i in range(len(g)):\n",
    "            if i+1<k: continue\n",
    "            feat=[]\n",
    "            for c in COLS_SIGNAL:\n",
    "                feat += [ m.loc[i,c] if pd.notna(m.loc[i,c]) else 0.0,\n",
    "                          s.loc[i,c] if pd.notna(s.loc[i,c]) else 0.0,\n",
    "                          ew.loc[i,c] if pd.notna(ew.loc[i,c]) else g.loc[i,c],\n",
    "                          rv.loc[i,c] if pd.notna(rv.loc[i,c]) else 0.0,\n",
    "                          L1.loc[i,c] if pd.notna(L1.loc[i,c]) else g.loc[i,c],\n",
    "                          L2.loc[i,c] if pd.notna(L2.loc[i,c]) else g.loc[i,c] ]\n",
    "            rows.append((feat, int(g.loc[i,COL_LABEL])))\n",
    "    X=np.array([f for f,_ in rows], dtype=np.float32) if rows else np.zeros((0,24), np.float32)\n",
    "    y=np.array([y for _,y in rows], dtype=np.int32) if rows else np.zeros((0,), np.int32)\n",
    "    X=np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return X,y\n",
    "\n",
    "def build_p2(df):\n",
    "    # Try RobustFeatureExtractor if present\n",
    "    try:\n",
    "        from RFE import RobustFeatureExtractor\n",
    "        rfe = RobustFeatureExtractor()\n",
    "        Xs=[]; Ys=[]\n",
    "        for sid,g in df.groupby(\"__stream_id__\"):\n",
    "            Xf = rfe.extract_from_dataframe(g[[COL_TIME]+COLS_SIGNAL])\n",
    "            yv = g[COL_LABEL].values\n",
    "            n  = min(len(Xf), len(yv))\n",
    "            Xs.append(np.asarray(Xf[:n], np.float32)); Ys.append(yv[:n].astype(np.int32))\n",
    "        X=np.vstack(Xs) if Xs else np.zeros((0,1), np.float32)\n",
    "        y=np.concatenate(Ys) if Ys else np.zeros((0,), np.int32)\n",
    "        X=np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        return X,y,\"RFE\"\n",
    "    except Exception:\n",
    "        # BASIC steady fallback (mean/std/min/max over sliding 15)\n",
    "        rows=[]\n",
    "        for sid,g in df.groupby(\"__stream_id__\"):\n",
    "            g=g.reset_index(drop=True)\n",
    "            roll = g[COLS_SIGNAL].rolling(15, min_periods=5)\n",
    "            mean = roll.mean(); std=roll.std().fillna(0.0); mn=roll.min(); mx=roll.max()\n",
    "            for i in range(len(g)):\n",
    "                feat=[]\n",
    "                for c in COLS_SIGNAL:\n",
    "                    feat += [ mean.loc[i,c] if pd.notna(mean.loc[i,c]) else g.loc[i,c],\n",
    "                              std.loc[i,c]  if pd.notna(std.loc[i,c])  else 0.0,\n",
    "                              mn.loc[i,c]   if pd.notna(mn.loc[i,c])   else g.loc[i,c],\n",
    "                              mx.loc[i,c]   if pd.notna(mx.loc[i,c])   else g.loc[i,c] ]\n",
    "                rows.append((feat, int(g.loc[i,COL_LABEL])))\n",
    "        X=np.array([f for f,_ in rows], np.float32); y=np.array([y for _,y in rows], np.int32)\n",
    "        X=np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        return X,y,\"BASIC\"\n",
    "\n",
    "# ---------- BUILD ----------\n",
    "X0_tr,y0_tr = build_p0(df_tr); X0_te,y0_te = build_p0(df_te)\n",
    "X1_tr,y1_tr = build_p1(df_tr, k=5); X1_te,y1_te = build_p1(df_te, k=5)\n",
    "X2_tr,y2_tr,mode2 = build_p2(df_tr); X2_te,y2_te,_ = build_p2(df_te)\n",
    "\n",
    "print(\"Training files used:\", [p.name for p in TRAIN_FILES])\n",
    "print(\"Phase shapes: P0\", X0_tr.shape, \" P1\", X1_tr.shape, \" P2\", X2_tr.shape, \"(steady:\", mode2, \")\")\n",
    "\n",
    "# ---------- FIT+EXPORT ----------\n",
    "def fit_export(tag, Xtr,ytr, Xte,yte):\n",
    "    scaler = StandardScaler().fit(Xtr)\n",
    "    Ztr = scaler.transform(Xtr); Zte = scaler.transform(Xte)\n",
    "\n",
    "    lr  = LogisticRegression(max_iter=1000, solver='lbfgs').fit(Ztr, ytr)\n",
    "    write_lr_header(tag, scaler, lr)\n",
    "\n",
    "    svc = LinearSVC().fit(Ztr, ytr)\n",
    "    write_svc_header(tag, scaler, svc)\n",
    "\n",
    "    rf  = RandomForestClassifier(n_estimators=25, max_depth=6, min_samples_leaf=2,\n",
    "                                 max_features=\"sqrt\", bootstrap=False, random_state=42).fit(Ztr, ytr)\n",
    "    write_rf_header(tag, scaler, rf)\n",
    "\n",
    "fit_export(\"P0\", X0_tr,y0_tr, X0_te,y0_te)\n",
    "fit_export(\"P1\", X1_tr,y1_tr, X1_te,y1_te)\n",
    "fit_export(\"P2\", X2_tr,y2_tr, X2_te,y2_te)\n",
    "\n",
    "print(\"✅ Headers written to ./include\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P27XWCAxS1ZI"
   },
   "source": [
    "# **new FE - dual mode**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBweW_8VUlB0"
   },
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TDR17hbjS3B7",
    "outputId": "9aaf2c83-a666-4315-b880-725f658f4870"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading Data...\n",
      "2. Training COLD Model (with SMOTE)...\n",
      "3. Training WARM Model (with SMOTE)...\n",
      "4. Writing headers...\n",
      "Done. Copy 'rfe_settings.h' and 'model_edge_dual.h' to your ESP32/src folder.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "TRAIN_FILES = [\n",
    "    \"./dataset/Train-set_1.xlsx\",\n",
    "    \"./dataset/Train-set_2.xlsx\"\n",
    "]\n",
    "\n",
    "ROLLING_WINDOWS = [5, 15]\n",
    "NUM_LAGS = 3\n",
    "WARMUP_PERIOD = 15\n",
    "\n",
    "# ================= FEATURE EXTRACTORS =================\n",
    "class ColdFeatureExtractor:\n",
    "    def __init__(self, df, time_col=\"Time\"):\n",
    "        self.df = df.copy()\n",
    "        if time_col in self.df.columns:\n",
    "            self.df[time_col] = pd.to_datetime(self.df[time_col], errors='coerce')\n",
    "        self.numeric_cols = [c for c in self.df.select_dtypes(include=np.number).columns if 'Label' not in c]\n",
    "\n",
    "    def extract(self):\n",
    "        out = pd.DataFrame(index=self.df.index)\n",
    "        df_num = self.df[self.numeric_cols]\n",
    "        for c in self.numeric_cols: out[c] = df_num[c]\n",
    "        if len(self.numeric_cols) >= 2:\n",
    "            for i, c1 in enumerate(self.numeric_cols):\n",
    "                for c2 in self.numeric_cols[i+1:]:\n",
    "                    out[f'inter_{c1}_x_{c2}'] = df_num[c1] * df_num[c2]\n",
    "        return out.fillna(0)\n",
    "\n",
    "class WarmFeatureExtractor:\n",
    "    def __init__(self, df, rolling_windows, num_lags):\n",
    "        self.df = df.copy()\n",
    "        self.rolling_windows = rolling_windows\n",
    "        self.num_lags = num_lags\n",
    "        self.numeric_cols = [c for c in self.df.select_dtypes(include=np.number).columns if 'Label' not in c]\n",
    "\n",
    "    def extract(self):\n",
    "        out = pd.DataFrame(index=self.df.index)\n",
    "        df_num = self.df[self.numeric_cols]\n",
    "        for c in self.numeric_cols: out[c] = df_num[c]\n",
    "        df_speed = df_num.diff()\n",
    "        for c in self.numeric_cols: out[f'speed_change_{c}'] = df_speed[c]\n",
    "        stats_list = ['mean', 'median', 'std', 'var', 'min', 'max']\n",
    "        for w in self.rolling_windows:\n",
    "            for c in self.numeric_cols:\n",
    "                r = df_num[c].rolling(window=w, min_periods=1)\n",
    "                for s in stats_list:\n",
    "                    try: out[f'rolling_{s}_{w}_{c}'] = getattr(r, s)().fillna(0)\n",
    "                    except: pass\n",
    "        for w in self.rolling_windows:\n",
    "            for c in self.numeric_cols:\n",
    "                r = df_speed[c].rolling(window=w, min_periods=1)\n",
    "                for s in ['mean', 'std']:\n",
    "                    out[f'rolling_{s}_{w}_speed_change_{c}'] = getattr(r, s)().fillna(0)\n",
    "        for i in range(1, self.num_lags + 1):\n",
    "            for c in self.numeric_cols: out[f'lag_{i}_{c}'] = df_num[c].shift(i)\n",
    "        span = self.rolling_windows[0]\n",
    "        for c in self.numeric_cols: out[f'ewma_{span}_{c}'] = df_num[c].ewm(span=span).mean()\n",
    "        if len(self.numeric_cols) >= 2:\n",
    "            for i, c1 in enumerate(self.numeric_cols):\n",
    "                for c2 in self.numeric_cols[i+1:]:\n",
    "                    out[f'inter_{c1}_x_{c2}'] = df_num[c1] * df_num[c2]\n",
    "        return out\n",
    "\n",
    "# ================= C++ GENERATORS =================\n",
    "def generate_specs_code(feature_names, raw_cols, array_name):\n",
    "    lines = []\n",
    "    stat_map = {'mean':0, 'median':1, 'std':2, 'var':3, 'min':4, 'max':5}\n",
    "    for feat in feature_names:\n",
    "        kind, stat, win, lag, ch1, ch2 = \"FEAT_UNKNOWN\", -1, 0, 0, -1, -1\n",
    "        if feat in raw_cols:\n",
    "            kind, ch1 = \"FEAT_RAW\", raw_cols.index(feat)\n",
    "        elif feat.startswith(\"inter_\"):\n",
    "            kind = \"FEAT_INTER\"\n",
    "            parts = feat[6:].split('_x_')\n",
    "            if len(parts)==2 and parts[0] in raw_cols and parts[1] in raw_cols:\n",
    "                ch1, ch2 = raw_cols.index(parts[0]), raw_cols.index(parts[1])\n",
    "        elif feat.startswith(\"speed_change_\"):\n",
    "            kind = \"FEAT_DIFF\"\n",
    "            col = feat.replace(\"speed_change_\", \"\")\n",
    "            if col in raw_cols: ch1 = raw_cols.index(col)\n",
    "        elif \"speed_change\" in feat and \"rolling_\" in feat:\n",
    "            kind = \"FEAT_ROLL_DIFF\"\n",
    "            parts = feat.split('_')\n",
    "            stat, win, col = stat_map.get(parts[1], -1), int(parts[2]), \"_\".join(parts[5:])\n",
    "            if col in raw_cols: ch1 = raw_cols.index(col)\n",
    "        elif feat.startswith(\"rolling_\"):\n",
    "            kind = \"FEAT_ROLL_RAW\"\n",
    "            parts = feat.split('_')\n",
    "            stat, win, col = stat_map.get(parts[1], -1), int(parts[2]), \"_\".join(parts[3:])\n",
    "            if col in raw_cols: ch1 = raw_cols.index(col)\n",
    "        elif feat.startswith(\"lag_\"):\n",
    "            kind = \"FEAT_LAG\"\n",
    "            parts = feat.split('_')\n",
    "            lag, col = int(parts[1]), \"_\".join(parts[2:])\n",
    "            if col in raw_cols: ch1 = raw_cols.index(col)\n",
    "        elif feat.startswith(\"ewma_\"):\n",
    "            kind = \"FEAT_EWMA\"\n",
    "            parts = feat.split('_')\n",
    "            win, col = int(parts[1]), \"_\".join(parts[2:])\n",
    "            if col in raw_cols: ch1 = raw_cols.index(col)\n",
    "        lines.append(f\"  {{ {kind}, {stat}, {win}, {lag}, {ch1}, {ch2} }}, // {feat}\")\n",
    "    return f\"static const FeatureSpec {array_name}[] = {{\\n\" + \"\\n\".join(lines) + \"\\n};\\n\"\n",
    "\n",
    "def export_rf_model(rf, scaler, prefix, filename):\n",
    "    all_left, all_right, all_feat, all_thresh, all_prob1 = [], [], [], [], []\n",
    "    offsets = [0]\n",
    "    curr_offset = 0\n",
    "\n",
    "    for est in rf.estimators_:\n",
    "        tree = est.tree_\n",
    "        for i in range(tree.node_count):\n",
    "            is_leaf = (tree.children_left[i] == tree.children_right[i])\n",
    "            all_thresh.append(tree.threshold[i])\n",
    "            all_feat.append(tree.feature[i])\n",
    "            if is_leaf:\n",
    "                all_left.append(-1); all_right.append(-1)\n",
    "                v = tree.value[i][0]\n",
    "                all_prob1.append(v[1] / v.sum() if v.sum() > 0 else 0)\n",
    "            else:\n",
    "                all_left.append(tree.children_left[i] + curr_offset)\n",
    "                all_right.append(tree.children_right[i] + curr_offset)\n",
    "                all_prob1.append(0)\n",
    "        curr_offset += tree.node_count\n",
    "        offsets.append(curr_offset)\n",
    "\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(f\"\\n// ===== MODEL: {prefix} =====\\n\")\n",
    "        f.write(f\"#define {prefix}_N_FEATURES {scaler.n_features_in_}\\n\")\n",
    "        f.write(f\"#define {prefix}_N_TREES {len(rf.estimators_)}\\n\")\n",
    "\n",
    "        f.write(f\"static const float {prefix}_SCALE_MEAN[] = {{ {', '.join(f'{x:.6f}' for x in scaler.mean_)} }};\\n\")\n",
    "        f.write(f\"static const float {prefix}_SCALE_STD[]  = {{ {', '.join(f'{x:.6f}' for x in scaler.scale_)} }};\\n\")\n",
    "\n",
    "        def w(n, t, d): f.write(f\"static const {t} {prefix}_{n}[] = {{ {', '.join(str(x) for x in d)} }};\\n\")\n",
    "        w(\"TREE_OFFSETS\", \"int\", offsets)\n",
    "        w(\"FEATURE\", \"int\", all_feat)\n",
    "        w(\"THRESHOLD\", \"float\", all_thresh)\n",
    "        w(\"LEFT\", \"int\", all_left)\n",
    "        w(\"RIGHT\", \"int\", all_right)\n",
    "        w(\"PROB1\", \"float\", all_prob1)\n",
    "\n",
    "# ================= MAIN =================\n",
    "def main():\n",
    "    print(\"1. Loading Data...\")\n",
    "    dfs = []\n",
    "    for f in TRAIN_FILES:\n",
    "        try:\n",
    "            d = pd.read_excel(f)\n",
    "            d['Label'] = pd.to_numeric(d.get('Label', 1), errors='coerce').fillna(1)\n",
    "            dfs.append(d)\n",
    "        except: pass\n",
    "\n",
    "    if not dfs:\n",
    "        print(\"Error: No data loaded. Check file paths!\")\n",
    "        return\n",
    "\n",
    "    df_all = pd.concat(dfs, ignore_index=True)\n",
    "    raw_cols = [c for c in df_all.select_dtypes(include=np.number).columns if c != 'Label']\n",
    "\n",
    "    sm = SMOTE(random_state=SEED)\n",
    "\n",
    "    # --- TRAIN COLD MODEL ---\n",
    "    print(\"2. Training COLD Model (with SMOTE)...\")\n",
    "    cold_ext = ColdFeatureExtractor(df_all)\n",
    "    X_cold = cold_ext.extract()\n",
    "    y_cold = df_all['Label']\n",
    "\n",
    "    X_cold_res, y_cold_res = sm.fit_resample(X_cold, y_cold)\n",
    "    scaler_cold = StandardScaler()\n",
    "    X_cold_s = scaler_cold.fit_transform(X_cold_res)\n",
    "\n",
    "    # n_jobs=1 ensures deterministic behavior\n",
    "    rf_cold = RandomForestClassifier(n_estimators=10, max_depth=6, random_state=SEED, n_jobs=1)\n",
    "    rf_cold.fit(X_cold_s, y_cold_res)\n",
    "\n",
    "    # --- TRAIN WARM MODEL ---\n",
    "    print(\"3. Training WARM Model (with SMOTE)...\")\n",
    "    warm_ext = WarmFeatureExtractor(df_all, ROLLING_WINDOWS, NUM_LAGS)\n",
    "    X_warm = warm_ext.extract()\n",
    "    mask = ~X_warm.isna().any(axis=1)\n",
    "    mask.iloc[:WARMUP_PERIOD] = False\n",
    "    X_warm_clean = X_warm[mask]\n",
    "    y_warm_clean = df_all['Label'][mask]\n",
    "\n",
    "    X_warm_res, y_warm_res = sm.fit_resample(X_warm_clean, y_warm_clean)\n",
    "    scaler_warm = StandardScaler()\n",
    "    X_warm_s = scaler_warm.fit_transform(X_warm_res)\n",
    "\n",
    "    # n_jobs=1 ensures deterministic behavior\n",
    "    rf_warm = RandomForestClassifier(n_estimators=20, max_depth=8, random_state=SEED, n_jobs=1)\n",
    "    rf_warm.fit(X_warm_s, y_warm_res)\n",
    "\n",
    "    # --- EXPORT ---\n",
    "    print(\"4. Writing headers...\")\n",
    "    if os.path.exists(\"rfe_settings.h\"): os.remove(\"rfe_settings.h\")\n",
    "    if os.path.exists(\"model_edge_dual.h\"): os.remove(\"model_edge_dual.h\")\n",
    "\n",
    "    with open(\"rfe_settings.h\", \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        for i, c in enumerate(raw_cols): f.write(f\"#define IDX_{c.upper().replace(' ','_')} {i}\\n\")\n",
    "        f.write(f\"#define NUM_RAW_INPUTS {len(raw_cols)}\\n\")\n",
    "        f.write(f\"#define WARMUP_PERIOD {WARMUP_PERIOD}\\n\")\n",
    "        f.write(f\"#define N_FEATURES_COLD {X_cold.shape[1]}\\n\")\n",
    "        f.write(f\"#define N_FEATURES_WARM {X_warm.shape[1]}\\n\\n\")\n",
    "        f.write(\"enum FeatureKind { FEAT_RAW, FEAT_DIFF, FEAT_ROLL_RAW, FEAT_ROLL_DIFF, FEAT_LAG, FEAT_EWMA, FEAT_INTER, FEAT_ROLL_TD, FEAT_UNKNOWN };\\n\")\n",
    "        f.write(\"typedef struct { FeatureKind kind; int stat; int window; int lag; int channel1; int channel2; } FeatureSpec;\\n\\n\")\n",
    "        f.write(generate_specs_code(X_cold.columns, raw_cols, \"FEATURE_SPECS_COLD\"))\n",
    "        f.write(generate_specs_code(X_warm.columns, raw_cols, \"FEATURE_SPECS_WARM\"))\n",
    "\n",
    "    with open(\"model_edge_dual.h\", \"w\") as f:\n",
    "        f.write(\"#pragma once\\n#include <stdint.h>\\n\")\n",
    "    export_rf_model(rf_cold, scaler_cold, \"RF_COLD\", \"model_edge_dual.h\")\n",
    "    export_rf_model(rf_warm, scaler_warm, \"RF_WARM\", \"model_edge_dual.h\")\n",
    "\n",
    "    print(\"Done. Copy 'rfe_settings.h' and 'model_edge_dual.h' to your ESP32/src folder.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSo2mQFsUnjn"
   },
   "source": [
    "## LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JH_gPTP8UpJf",
    "outputId": "7a7d5065-f3bd-4563-f6a2-6fc08b9f80c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading Data...\n",
      "2. Training COLD Model (LR + SMOTE)...\n",
      "3. Training WARM Model (LR + SMOTE)...\n",
      "4. Writing headers...\n",
      "Done. Ready for ESP32 (LR Version).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "# Enforce reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "TRAIN_FILES = [\n",
    "    \"./dataset/Train-set_1.xlsx\",\n",
    "    \"./dataset/Train-set_2.xlsx\"\n",
    "]\n",
    "\n",
    "ROLLING_WINDOWS = [5, 15]\n",
    "NUM_LAGS = 3\n",
    "WARMUP_PERIOD = 15\n",
    "\n",
    "# ================= FEATURE EXTRACTORS =================\n",
    "class ColdFeatureExtractor:\n",
    "    def __init__(self, df, time_col=\"Time\"):\n",
    "        self.df = df.copy()\n",
    "        if time_col in self.df.columns:\n",
    "            self.df[time_col] = pd.to_datetime(self.df[time_col], errors='coerce')\n",
    "        self.numeric_cols = [c for c in self.df.select_dtypes(include=np.number).columns if 'Label' not in c]\n",
    "\n",
    "    def extract(self):\n",
    "        out = pd.DataFrame(index=self.df.index)\n",
    "        df_num = self.df[self.numeric_cols]\n",
    "        for c in self.numeric_cols: out[c] = df_num[c]\n",
    "        if len(self.numeric_cols) >= 2:\n",
    "            for i, c1 in enumerate(self.numeric_cols):\n",
    "                for c2 in self.numeric_cols[i+1:]:\n",
    "                    out[f'inter_{c1}_x_{c2}'] = df_num[c1] * df_num[c2]\n",
    "        return out.fillna(0)\n",
    "\n",
    "class WarmFeatureExtractor:\n",
    "    def __init__(self, df, rolling_windows, num_lags):\n",
    "        self.df = df.copy()\n",
    "        self.rolling_windows = rolling_windows\n",
    "        self.num_lags = num_lags\n",
    "        self.numeric_cols = [c for c in self.df.select_dtypes(include=np.number).columns if 'Label' not in c]\n",
    "\n",
    "    def extract(self):\n",
    "        out = pd.DataFrame(index=self.df.index)\n",
    "        df_num = self.df[self.numeric_cols]\n",
    "        for c in self.numeric_cols: out[c] = df_num[c]\n",
    "        df_speed = df_num.diff()\n",
    "        for c in self.numeric_cols: out[f'speed_change_{c}'] = df_speed[c]\n",
    "        stats_list = ['mean', 'median', 'std', 'var', 'min', 'max']\n",
    "        for w in self.rolling_windows:\n",
    "            for c in self.numeric_cols:\n",
    "                r = df_num[c].rolling(window=w, min_periods=1)\n",
    "                for s in stats_list:\n",
    "                    try: out[f'rolling_{s}_{w}_{c}'] = getattr(r, s)().fillna(0)\n",
    "                    except: pass\n",
    "        for w in self.rolling_windows:\n",
    "            for c in self.numeric_cols:\n",
    "                r = df_speed[c].rolling(window=w, min_periods=1)\n",
    "                for s in ['mean', 'std']:\n",
    "                    out[f'rolling_{s}_{w}_speed_change_{c}'] = getattr(r, s)().fillna(0)\n",
    "        for i in range(1, self.num_lags + 1):\n",
    "            for c in self.numeric_cols: out[f'lag_{i}_{c}'] = df_num[c].shift(i)\n",
    "        span = self.rolling_windows[0]\n",
    "        for c in self.numeric_cols: out[f'ewma_{span}_{c}'] = df_num[c].ewm(span=span).mean()\n",
    "        if len(self.numeric_cols) >= 2:\n",
    "            for i, c1 in enumerate(self.numeric_cols):\n",
    "                for c2 in self.numeric_cols[i+1:]:\n",
    "                    out[f'inter_{c1}_x_{c2}'] = df_num[c1] * df_num[c2]\n",
    "        return out\n",
    "\n",
    "# ================= C++ GENERATORS =================\n",
    "def generate_specs_code(feature_names, raw_cols, array_name):\n",
    "    lines = []\n",
    "    stat_map = {'mean':0, 'median':1, 'std':2, 'var':3, 'min':4, 'max':5}\n",
    "    for feat in feature_names:\n",
    "        kind, stat, win, lag, ch1, ch2 = \"FEAT_UNKNOWN\", -1, 0, 0, -1, -1\n",
    "        if feat in raw_cols:\n",
    "            kind, ch1 = \"FEAT_RAW\", raw_cols.index(feat)\n",
    "        elif feat.startswith(\"inter_\"):\n",
    "            kind = \"FEAT_INTER\"\n",
    "            parts = feat[6:].split('_x_')\n",
    "            if len(parts)==2 and parts[0] in raw_cols and parts[1] in raw_cols:\n",
    "                ch1, ch2 = raw_cols.index(parts[0]), raw_cols.index(parts[1])\n",
    "        elif feat.startswith(\"speed_change_\"):\n",
    "            kind = \"FEAT_DIFF\"\n",
    "            col = feat.replace(\"speed_change_\", \"\")\n",
    "            if col in raw_cols: ch1 = raw_cols.index(col)\n",
    "        elif \"speed_change\" in feat and \"rolling_\" in feat:\n",
    "            kind = \"FEAT_ROLL_DIFF\"\n",
    "            parts = feat.split('_')\n",
    "            stat, win, col = stat_map.get(parts[1], -1), int(parts[2]), \"_\".join(parts[5:])\n",
    "            if col in raw_cols: ch1 = raw_cols.index(col)\n",
    "        elif feat.startswith(\"rolling_\"):\n",
    "            kind = \"FEAT_ROLL_RAW\"\n",
    "            parts = feat.split('_')\n",
    "            stat, win, col = stat_map.get(parts[1], -1), int(parts[2]), \"_\".join(parts[3:])\n",
    "            if col in raw_cols: ch1 = raw_cols.index(col)\n",
    "        elif feat.startswith(\"lag_\"):\n",
    "            kind = \"FEAT_LAG\"\n",
    "            parts = feat.split('_')\n",
    "            lag, col = int(parts[1]), \"_\".join(parts[2:])\n",
    "            if col in raw_cols: ch1 = raw_cols.index(col)\n",
    "        elif feat.startswith(\"ewma_\"):\n",
    "            kind = \"FEAT_EWMA\"\n",
    "            parts = feat.split('_')\n",
    "            win, col = int(parts[1]), \"_\".join(parts[2:])\n",
    "            if col in raw_cols: ch1 = raw_cols.index(col)\n",
    "        lines.append(f\"  {{ {kind}, {stat}, {win}, {lag}, {ch1}, {ch2} }}, // {feat}\")\n",
    "    return f\"static const FeatureSpec {array_name}[] = {{\\n\" + \"\\n\".join(lines) + \"\\n};\\n\"\n",
    "\n",
    "def export_lr_model(lr, scaler, prefix, filename):\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(f\"\\n// ===== MODEL: {prefix} (Logistic Regression) =====\\n\")\n",
    "        f.write(f\"#define {prefix}_N_FEATURES {scaler.n_features_in_}\\n\")\n",
    "        f.write(f\"static const float {prefix}_SCALE_MEAN[] = {{ {', '.join(f'{x:.6f}' for x in scaler.mean_)} }};\\n\")\n",
    "        f.write(f\"static const float {prefix}_SCALE_STD[]  = {{ {', '.join(f'{x:.6f}' for x in scaler.scale_)} }};\\n\")\n",
    "        coefs = lr.coef_[0]\n",
    "        f.write(f\"static const float {prefix}_COEF[] = {{ {', '.join(f'{x:.6f}' for x in coefs)} }};\\n\")\n",
    "        bias = lr.intercept_[0]\n",
    "        f.write(f\"static const float {prefix}_BIAS = {bias:.6f};\\n\")\n",
    "\n",
    "# ================= MAIN =================\n",
    "def main():\n",
    "    print(\"1. Loading Data...\")\n",
    "    dfs = []\n",
    "    for f in TRAIN_FILES:\n",
    "        try:\n",
    "            d = pd.read_excel(f)\n",
    "            d['Label'] = pd.to_numeric(d.get('Label', 1), errors='coerce').fillna(1)\n",
    "            dfs.append(d)\n",
    "        except: pass\n",
    "\n",
    "    if not dfs:\n",
    "        print(\"Error: No data loaded.\")\n",
    "        return\n",
    "\n",
    "    df_all = pd.concat(dfs, ignore_index=True)\n",
    "    raw_cols = [c for c in df_all.select_dtypes(include=np.number).columns if c != 'Label']\n",
    "\n",
    "    sm = SMOTE(random_state=SEED)\n",
    "\n",
    "    # --- TRAIN COLD MODEL (LR) ---\n",
    "    print(\"2. Training COLD Model (LR + SMOTE)...\")\n",
    "    cold_ext = ColdFeatureExtractor(df_all)\n",
    "    X_cold = cold_ext.extract()\n",
    "    y_cold = df_all['Label']\n",
    "\n",
    "    # APPLY SMOTE\n",
    "    X_cold_res, y_cold_res = sm.fit_resample(X_cold, y_cold)\n",
    "\n",
    "    scaler_cold = StandardScaler()\n",
    "    X_cold_s = scaler_cold.fit_transform(X_cold_res)\n",
    "\n",
    "    lr_cold = LogisticRegression(random_state=SEED, max_iter=1000)\n",
    "    lr_cold.fit(X_cold_s, y_cold_res)\n",
    "\n",
    "    # --- TRAIN WARM MODEL (LR) ---\n",
    "    print(\"3. Training WARM Model (LR + SMOTE)...\")\n",
    "    warm_ext = WarmFeatureExtractor(df_all, ROLLING_WINDOWS, NUM_LAGS)\n",
    "    X_warm = warm_ext.extract()\n",
    "    mask = ~X_warm.isna().any(axis=1)\n",
    "    mask.iloc[:WARMUP_PERIOD] = False\n",
    "    X_warm_clean = X_warm[mask]\n",
    "    y_warm_clean = df_all['Label'][mask]\n",
    "\n",
    "    # APPLY SMOTE\n",
    "    X_warm_res, y_warm_res = sm.fit_resample(X_warm_clean, y_warm_clean)\n",
    "\n",
    "    scaler_warm = StandardScaler()\n",
    "    X_warm_s = scaler_warm.fit_transform(X_warm_res)\n",
    "\n",
    "    lr_warm = LogisticRegression(random_state=SEED, max_iter=1000)\n",
    "    lr_warm.fit(X_warm_s, y_warm_res)\n",
    "\n",
    "    # --- EXPORT ---\n",
    "    print(\"4. Writing headers...\")\n",
    "    if os.path.exists(\"rfe_settings.h\"): os.remove(\"rfe_settings.h\")\n",
    "    if os.path.exists(\"model_edge_dual.h\"): os.remove(\"model_edge_dual.h\")\n",
    "\n",
    "    with open(\"rfe_settings.h\", \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        for i, c in enumerate(raw_cols): f.write(f\"#define IDX_{c.upper().replace(' ','_')} {i}\\n\")\n",
    "        f.write(f\"#define NUM_RAW_INPUTS {len(raw_cols)}\\n\")\n",
    "        f.write(f\"#define WARMUP_PERIOD {WARMUP_PERIOD}\\n\")\n",
    "        f.write(f\"#define N_FEATURES_COLD {X_cold.shape[1]}\\n\")\n",
    "        f.write(f\"#define N_FEATURES_WARM {X_warm.shape[1]}\\n\\n\")\n",
    "        f.write(\"enum FeatureKind { FEAT_RAW, FEAT_DIFF, FEAT_ROLL_RAW, FEAT_ROLL_DIFF, FEAT_LAG, FEAT_EWMA, FEAT_INTER, FEAT_ROLL_TD, FEAT_UNKNOWN };\\n\")\n",
    "        f.write(\"typedef struct { FeatureKind kind; int stat; int window; int lag; int channel1; int channel2; } FeatureSpec;\\n\\n\")\n",
    "        f.write(generate_specs_code(X_cold.columns, raw_cols, \"FEATURE_SPECS_COLD\"))\n",
    "        f.write(generate_specs_code(X_warm.columns, raw_cols, \"FEATURE_SPECS_WARM\"))\n",
    "\n",
    "    with open(\"model_edge_dual.h\", \"w\") as f:\n",
    "        f.write(\"#pragma once\\n#include <stdint.h>\\n\")\n",
    "    export_lr_model(lr_cold, scaler_cold, \"LR_COLD\", \"model_edge_dual.h\")\n",
    "    export_lr_model(lr_warm, scaler_warm, \"LR_WARM\", \"model_edge_dual.h\")\n",
    "\n",
    "    print(\"Done. Ready for ESP32 (LR Version).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3iQmxBMRcqc4"
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BCV8GpmrcrbJ",
    "outputId": "78eb9fc0-1263-4b55-942a-0c8f31e2e34e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import random\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "TRAIN_FILES = [\n",
    "    \"./dataset/Train-set_1.xlsx\",\n",
    "    \"./dataset/Train-set_2.xlsx\"\n",
    "]\n",
    "\n",
    "ROLLING_WINDOWS = [5, 15]\n",
    "NUM_LAGS = 3\n",
    "WARMUP_PERIOD = 15\n",
    "\n",
    "# ================= FEATURE EXTRACTORS =================\n",
    "class ColdFeatureExtractor:\n",
    "    def __init__(self, df, time_col=\"Time\"):\n",
    "        self.df = df.copy()\n",
    "        if time_col in self.df.columns:\n",
    "            self.df[time_col] = pd.to_datetime(self.df[time_col], errors='coerce')\n",
    "        self.numeric_cols = [c for c in self.df.select_dtypes(include=np.number).columns if 'Label' not in c]\n",
    "\n",
    "    def extract(self):\n",
    "        out = pd.DataFrame(index=self.df.index)\n",
    "        df_num = self.df[self.numeric_cols]\n",
    "        for c in self.numeric_cols: out[c] = df_num[c]\n",
    "        if len(self.numeric_cols) >= 2:\n",
    "            for i, c1 in enumerate(self.numeric_cols):\n",
    "                for c2 in self.numeric_cols[i+1:]:\n",
    "                    out[f'inter_{c1}_x_{c2}'] = df_num[c1] * df_num[c2]\n",
    "        return out.fillna(0)\n",
    "\n",
    "class WarmFeatureExtractor:\n",
    "    def __init__(self, df, rolling_windows, num_lags):\n",
    "        self.df = df.copy()\n",
    "        self.rolling_windows = rolling_windows\n",
    "        self.num_lags = num_lags\n",
    "        self.numeric_cols = [c for c in self.df.select_dtypes(include=np.number).columns if 'Label' not in c]\n",
    "\n",
    "    def extract(self):\n",
    "        out = pd.DataFrame(index=self.df.index)\n",
    "        df_num = self.df[self.numeric_cols]\n",
    "        for c in self.numeric_cols: out[c] = df_num[c]\n",
    "        df_speed = df_num.diff()\n",
    "        for c in self.numeric_cols: out[f'speed_change_{c}'] = df_speed[c]\n",
    "        stats_list = ['mean', 'median', 'std', 'var', 'min', 'max']\n",
    "        for w in self.rolling_windows:\n",
    "            for c in self.numeric_cols:\n",
    "                r = df_num[c].rolling(window=w, min_periods=1)\n",
    "                for s in stats_list:\n",
    "                    try: out[f'rolling_{s}_{w}_{c}'] = getattr(r, s)().fillna(0)\n",
    "                    except: pass\n",
    "        for w in self.rolling_windows:\n",
    "            for c in self.numeric_cols:\n",
    "                r = df_speed[c].rolling(window=w, min_periods=1)\n",
    "                for s in ['mean', 'std']:\n",
    "                    out[f'rolling_{s}_{w}_speed_change_{c}'] = getattr(r, s)().fillna(0)\n",
    "        for i in range(1, self.num_lags + 1):\n",
    "            for c in self.numeric_cols: out[f'lag_{i}_{c}'] = df_num[c].shift(i)\n",
    "        span = self.rolling_windows[0]\n",
    "        for c in self.numeric_cols: out[f'ewma_{span}_{c}'] = df_num[c].ewm(span=span).mean()\n",
    "        if len(self.numeric_cols) >= 2:\n",
    "            for i, c1 in enumerate(self.numeric_cols):\n",
    "                for c2 in self.numeric_cols[i+1:]:\n",
    "                    out[f'inter_{c1}_x_{c2}'] = df_num[c1] * df_num[c2]\n",
    "        return out\n",
    "\n",
    "# ================= C++ GENERATORS =================\n",
    "def generate_specs_code(feature_names, raw_cols, array_name):\n",
    "    lines = []\n",
    "    stat_map = {'mean':0, 'median':1, 'std':2, 'var':3, 'min':4, 'max':5}\n",
    "    for feat in feature_names:\n",
    "        kind, stat, win, lag, ch1, ch2 = \"FEAT_UNKNOWN\", -1, 0, 0, -1, -1\n",
    "        if feat in raw_cols:\n",
    "            kind, ch1 = \"FEAT_RAW\", raw_cols.index(feat)\n",
    "        elif feat.startswith(\"inter_\"):\n",
    "            kind = \"FEAT_INTER\"\n",
    "            parts = feat[6:].split('_x_')\n",
    "            if len(parts)==2 and parts[0] in raw_cols and parts[1] in raw_cols:\n",
    "                ch1, ch2 = raw_cols.index(parts[0]), raw_cols.index(parts[1])\n",
    "        elif feat.startswith(\"speed_change_\"):\n",
    "            kind = \"FEAT_DIFF\"\n",
    "            col = feat.replace(\"speed_change_\", \"\")\n",
    "            if col in raw_cols: ch1 = raw_cols.index(col)\n",
    "        elif \"speed_change\" in feat and \"rolling_\" in feat:\n",
    "            kind = \"FEAT_ROLL_DIFF\"\n",
    "            parts = feat.split('_')\n",
    "            stat, win, col = stat_map.get(parts[1], -1), int(parts[2]), \"_\".join(parts[5:])\n",
    "            if col in raw_cols: ch1 = raw_cols.index(col)\n",
    "        elif feat.startswith(\"rolling_\"):\n",
    "            kind = \"FEAT_ROLL_RAW\"\n",
    "            parts = feat.split('_')\n",
    "            stat, win, col = stat_map.get(parts[1], -1), int(parts[2]), \"_\".join(parts[3:])\n",
    "            if col in raw_cols: ch1 = raw_cols.index(col)\n",
    "        elif feat.startswith(\"lag_\"):\n",
    "            kind = \"FEAT_LAG\"\n",
    "            parts = feat.split('_')\n",
    "            lag, col = int(parts[1]), \"_\".join(parts[2:])\n",
    "            if col in raw_cols: ch1 = raw_cols.index(col)\n",
    "        elif feat.startswith(\"ewma_\"):\n",
    "            kind = \"FEAT_EWMA\"\n",
    "            parts = feat.split('_')\n",
    "            win, col = int(parts[1]), \"_\".join(parts[2:])\n",
    "            if col in raw_cols: ch1 = raw_cols.index(col)\n",
    "        lines.append(f\"  {{ {kind}, {stat}, {win}, {lag}, {ch1}, {ch2} }}, // {feat}\")\n",
    "    return f\"static const FeatureSpec {array_name}[] = {{\\n\" + \"\\n\".join(lines) + \"\\n};\\n\"\n",
    "\n",
    "def export_svm_model(svm, scaler, prefix, filename):\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(f\"\\n// ===== MODEL: {prefix} (Linear SVM) =====\\n\")\n",
    "        f.write(f\"#define {prefix}_N_FEATURES {scaler.n_features_in_}\\n\")\n",
    "        f.write(f\"static const float {prefix}_SCALE_MEAN[] = {{ {', '.join(f'{x:.6f}' for x in scaler.mean_)} }};\\n\")\n",
    "        f.write(f\"static const float {prefix}_SCALE_STD[]  = {{ {', '.join(f'{x:.6f}' for x in scaler.scale_)} }};\\n\")\n",
    "        coefs = svm.coef_[0]\n",
    "        f.write(f\"static const float {prefix}_COEF[] = {{ {', '.join(f'{x:.6f}' for x in coefs)} }};\\n\")\n",
    "        bias = svm.intercept_[0]\n",
    "        f.write(f\"static const float {prefix}_BIAS = {bias:.6f};\\n\")\n",
    "        pa = svm.probA_[0] if hasattr(svm, 'probA_') else 0.0\n",
    "        pb = svm.probB_[0] if hasattr(svm, 'probB_') else 0.0\n",
    "        f.write(f\"static const float {prefix}_PROB_A = {pa:.6f};\\n\")\n",
    "        f.write(f\"static const float {prefix}_PROB_B = {pb:.6f};\\n\")\n",
    "\n",
    "# ================= MAIN =================\n",
    "def main():\n",
    "    print(\"1. Loading Data...\")\n",
    "    dfs = []\n",
    "    for f in TRAIN_FILES:\n",
    "        try:\n",
    "            d = pd.read_excel(f)\n",
    "            d['Label'] = pd.to_numeric(d.get('Label', 1), errors='coerce').fillna(1)\n",
    "            dfs.append(d)\n",
    "        except: pass\n",
    "\n",
    "    if not dfs:\n",
    "        print(\"Error: No data loaded.\")\n",
    "        return\n",
    "\n",
    "    df_all = pd.concat(dfs, ignore_index=True)\n",
    "    raw_cols = [c for c in df_all.select_dtypes(include=np.number).columns if c != 'Label']\n",
    "\n",
    "    sm = SMOTE(random_state=SEED)\n",
    "\n",
    "    # --- TRAIN COLD MODEL (SVM) ---\n",
    "    print(\"2. Training COLD Model (SVM + SMOTE)...\")\n",
    "    cold_ext = ColdFeatureExtractor(df_all)\n",
    "    X_cold = cold_ext.extract()\n",
    "    y_cold = df_all['Label']\n",
    "\n",
    "    X_cold_res, y_cold_res = sm.fit_resample(X_cold, y_cold)\n",
    "    scaler_cold = StandardScaler()\n",
    "    X_cold_s = scaler_cold.fit_transform(X_cold_res)\n",
    "\n",
    "    svm_cold = SVC(kernel='linear', probability=True, random_state=SEED)\n",
    "    svm_cold.fit(X_cold_s, y_cold_res)\n",
    "\n",
    "    # --- TRAIN WARM MODEL (SVM) ---\n",
    "    print(\"3. Training WARM Model (SVM + SMOTE)...\")\n",
    "    warm_ext = WarmFeatureExtractor(df_all, ROLLING_WINDOWS, NUM_LAGS)\n",
    "    X_warm = warm_ext.extract()\n",
    "    mask = ~X_warm.isna().any(axis=1)\n",
    "    mask.iloc[:WARMUP_PERIOD] = False\n",
    "    X_warm_clean = X_warm[mask]\n",
    "    y_warm_clean = df_all['Label'][mask]\n",
    "\n",
    "    X_warm_res, y_warm_res = sm.fit_resample(X_warm_clean, y_warm_clean)\n",
    "    scaler_warm = StandardScaler()\n",
    "    X_warm_s = scaler_warm.fit_transform(X_warm_res)\n",
    "\n",
    "    svm_warm = SVC(kernel='linear', probability=True, random_state=SEED)\n",
    "    svm_warm.fit(X_warm_s, y_warm_res)\n",
    "\n",
    "    # --- EXPORT ---\n",
    "    print(\"4. Writing headers...\")\n",
    "    if os.path.exists(\"rfe_settings.h\"): os.remove(\"rfe_settings.h\")\n",
    "    if os.path.exists(\"model_edge_dual.h\"): os.remove(\"model_edge_dual.h\")\n",
    "\n",
    "    with open(\"rfe_settings.h\", \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        for i, c in enumerate(raw_cols): f.write(f\"#define IDX_{c.upper().replace(' ','_')} {i}\\n\")\n",
    "        f.write(f\"#define NUM_RAW_INPUTS {len(raw_cols)}\\n\")\n",
    "        f.write(f\"#define WARMUP_PERIOD {WARMUP_PERIOD}\\n\")\n",
    "        f.write(f\"#define N_FEATURES_COLD {X_cold.shape[1]}\\n\")\n",
    "        f.write(f\"#define N_FEATURES_WARM {X_warm.shape[1]}\\n\\n\")\n",
    "        f.write(\"enum FeatureKind { FEAT_RAW, FEAT_DIFF, FEAT_ROLL_RAW, FEAT_ROLL_DIFF, FEAT_LAG, FEAT_EWMA, FEAT_INTER, FEAT_ROLL_TD, FEAT_UNKNOWN };\\n\")\n",
    "        f.write(\"typedef struct { FeatureKind kind; int stat; int window; int lag; int channel1; int channel2; } FeatureSpec;\\n\\n\")\n",
    "        f.write(generate_specs_code(X_cold.columns, raw_cols, \"FEATURE_SPECS_COLD\"))\n",
    "        f.write(generate_specs_code(X_warm.columns, raw_cols, \"FEATURE_SPECS_WARM\"))\n",
    "\n",
    "    with open(\"model_edge_dual.h\", \"w\") as f:\n",
    "        f.write(\"#pragma once\\n#include <stdint.h>\\n\")\n",
    "    export_svm_model(svm_cold, scaler_cold, \"SVM_COLD\", \"model_edge_dual.h\")\n",
    "    export_svm_model(svm_warm, scaler_warm, \"SVM_WARM\", \"model_edge_dual.h\")\n",
    "\n",
    "    print(\"Done. Ready for ESP32 (SVM Version).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64VtJRa8hX52"
   },
   "source": [
    "# tsassuse (new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5EoN4V8w1QcU",
    "outputId": "18f8c437-aab5-41ec-e4ac-bdb2190408c3"
   },
   "outputs": [],
   "source": [
    "!uv add tsassure_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODlJBQSfmFsI"
   },
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xuRAi3_zhZg6",
    "outputId": "87b16aa0-f117-46d4-dccd-74ceb91b45a3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "TRAIN_FILES = [\n",
    "    \"./dataset/Train-set_1.xlsx\",\n",
    "    \"./dataset/Train-set_2.xlsx\"\n",
    "]\n",
    "\n",
    "class TsAssureExtractor:\n",
    "    def __init__(self, df, main_col_idx=0):\n",
    "        self.raw_cols = [c for c in df.select_dtypes(include=np.number).columns if 'Label' not in c]\n",
    "        self.main_col = self.raw_cols[main_col_idx]\n",
    "        self.correlated_pairs = []\n",
    "\n",
    "        print(f\"Finding correlations relative to main column: '{self.main_col}'...\")\n",
    "        candidates = []\n",
    "        for col in self.raw_cols:\n",
    "            if col != self.main_col:\n",
    "                corr = df[self.main_col].corr(df[col])\n",
    "                if abs(corr) > 0.7:\n",
    "                    candidates.append(col)\n",
    "\n",
    "        for i, c1 in enumerate(candidates):\n",
    "            for c2 in candidates[i+1:]:\n",
    "                corr = df[c1].corr(df[c2])\n",
    "                if abs(corr) > 0.65:\n",
    "                    self.correlated_pairs.append((c1, c2))\n",
    "\n",
    "    def extract(self, df):\n",
    "        out = pd.DataFrame(index=df.index)\n",
    "        df_num = df[self.raw_cols]\n",
    "        col_0 = df_num[self.main_col]\n",
    "\n",
    "        # 1. Raw & DiffMain\n",
    "        out[self.main_col] = col_0\n",
    "        for col in self.raw_cols:\n",
    "            if col != self.main_col:\n",
    "                out[col] = df_num[col]\n",
    "                out[f'DiffMain_{col}'] = col_0 - df_num[col]\n",
    "\n",
    "        # 2. Speed\n",
    "        df_speed = df_num.diff()\n",
    "        for col in self.raw_cols:\n",
    "            out[f'speed_change_{col}'] = df_speed[col]\n",
    "\n",
    "        # 3. PRD\n",
    "        prev_0 = col_0.shift(1)\n",
    "        mean_val = (col_0 + prev_0) * 0.5\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            prd = abs(col_0 - prev_0) / mean_val\n",
    "        out['PRD'] = prd.fillna(0)\n",
    "\n",
    "        # 4. Pairs\n",
    "        for c1, c2 in self.correlated_pairs:\n",
    "            out[f'DiffPair_{c1}_{c2}'] = df_num[c1] - df_num[c2]\n",
    "\n",
    "        return out.fillna(0)\n",
    "\n",
    "def generate_tsassure_settings(extractor, raw_cols):\n",
    "    lines = []\n",
    "    lines.append(f\"#define NUM_RAW_INPUTS {len(raw_cols)}\")\n",
    "    col_map = {name: i for i, name in enumerate(raw_cols)}\n",
    "    lines.append(f\"#define IDX_MAIN_COL {col_map[extractor.main_col]}\")\n",
    "    lines.append(f\"#define NUM_PAIRS {len(extractor.correlated_pairs)}\")\n",
    "    pairs_str = \"\"\n",
    "    for c1, c2 in extractor.correlated_pairs:\n",
    "        pairs_str += f\"  {{ {col_map[c1]}, {col_map[c2]} }}, // {c1}-{c2}\\n\"\n",
    "    return lines, pairs_str\n",
    "\n",
    "def export_rf_model(rf, scaler, prefix, filename):\n",
    "    all_left, all_right, all_feat, all_thresh, all_prob1 = [], [], [], [], []\n",
    "    offsets = [0]\n",
    "    curr_offset = 0\n",
    "\n",
    "    for est in rf.estimators_:\n",
    "        tree = est.tree_\n",
    "        for i in range(tree.node_count):\n",
    "            is_leaf = (tree.children_left[i] == tree.children_right[i])\n",
    "            all_thresh.append(tree.threshold[i])\n",
    "            all_feat.append(tree.feature[i])\n",
    "            if is_leaf:\n",
    "                all_left.append(-1); all_right.append(-1)\n",
    "                v = tree.value[i][0]\n",
    "                all_prob1.append(v[1] / v.sum() if v.sum() > 0 else 0)\n",
    "            else:\n",
    "                all_left.append(tree.children_left[i] + curr_offset)\n",
    "                all_right.append(tree.children_right[i] + curr_offset)\n",
    "                all_prob1.append(0)\n",
    "        curr_offset += tree.node_count\n",
    "        offsets.append(curr_offset)\n",
    "\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(f\"\\n// ===== MODEL: {prefix} =====\\n\")\n",
    "        f.write(f\"#define {prefix}_N_FEATURES {scaler.n_features_in_}\\n\")\n",
    "        f.write(f\"#define {prefix}_N_TREES {len(rf.estimators_)}\\n\")\n",
    "        f.write(f\"static const float {prefix}_SCALE_MEAN[] = {{ {', '.join(f'{x:.6f}' for x in scaler.mean_)} }};\\n\")\n",
    "        f.write(f\"static const float {prefix}_SCALE_STD[]  = {{ {', '.join(f'{x:.6f}' for x in scaler.scale_)} }};\\n\")\n",
    "\n",
    "        def w(n, t, d): f.write(f\"static const {t} {prefix}_{n}[] = {{ {', '.join(str(x) for x in d)} }};\\n\")\n",
    "        w(\"TREE_OFFSETS\", \"int\", offsets)\n",
    "        w(\"FEATURE\", \"int\", all_feat)\n",
    "        w(\"THRESHOLD\", \"float\", all_thresh)\n",
    "        w(\"LEFT\", \"int\", all_left)\n",
    "        w(\"RIGHT\", \"int\", all_right)\n",
    "        w(\"PROB1\", \"float\", all_prob1)\n",
    "\n",
    "def main():\n",
    "    print(\"1. Loading Data...\")\n",
    "    dfs = []\n",
    "    for f in TRAIN_FILES:\n",
    "        try:\n",
    "            d = pd.read_excel(f)\n",
    "            d['Label'] = pd.to_numeric(d.get('Label', 1), errors='coerce').fillna(1)\n",
    "            dfs.append(d)\n",
    "        except: pass\n",
    "    if not dfs: return\n",
    "    df_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    print(\"2. Training TsAssure Model (RF with SMOTE)...\")\n",
    "    ext = TsAssureExtractor(df_all, main_col_idx=0)\n",
    "    X = ext.extract(df_all)\n",
    "    y = df_all['Label']\n",
    "\n",
    "    sm = SMOTE(random_state=SEED)\n",
    "    X_res, y_res = sm.fit_resample(X, y)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_s = scaler.fit_transform(X_res)\n",
    "\n",
    "    # Single Thread for reproducibility\n",
    "    rf = RandomForestClassifier(n_estimators=20, max_depth=8, random_state=SEED, n_jobs=1)\n",
    "    rf.fit(X_s, y_res)\n",
    "\n",
    "    print(\"3. Writing headers...\")\n",
    "    if os.path.exists(\"tsassure_settings.h\"): os.remove(\"tsassure_settings.h\")\n",
    "    if os.path.exists(\"model_edge.h\"): os.remove(\"model_edge.h\")\n",
    "\n",
    "    setting_lines, pairs_str = generate_tsassure_settings(ext, ext.raw_cols)\n",
    "\n",
    "    with open(\"tsassure_settings.h\", \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        for i, c in enumerate(ext.raw_cols): f.write(f\"#define IDX_{c.upper().replace(' ','_')} {i}\\n\")\n",
    "        for line in setting_lines: f.write(line + \"\\n\")\n",
    "        f.write(f\"\\nstatic const int CORR_PAIRS[NUM_PAIRS > 0 ? NUM_PAIRS : 1][2] = {{\\n{pairs_str}}};\\n\")\n",
    "        f.write(f\"\\n#define TS_N_FEATURES {X.shape[1]}\\n\")\n",
    "\n",
    "    with open(\"model_edge.h\", \"w\") as f:\n",
    "        f.write(\"#pragma once\\n#include <stdint.h>\\n\")\n",
    "    export_rf_model(rf, scaler, \"RF\", \"model_edge.h\")\n",
    "\n",
    "    print(f\"Done. Features: {X.shape[1]}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Qn7QYdJpl2G"
   },
   "source": [
    "## LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uZ7KDk58pmok",
    "outputId": "1d6d9dbd-3d25-413c-9d4a-90809c73774e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "TRAIN_FILES = [\n",
    "    \"/content/dataset/Train-set_1.xlsx\",\n",
    "    \"/content/dataset/Train-set_2.xlsx\"\n",
    "]\n",
    "\n",
    "# ================= TSASSURE EXTRACTOR =================\n",
    "class TsAssureExtractor:\n",
    "    def __init__(self, df, main_col_idx=0):\n",
    "        self.raw_cols = [c for c in df.select_dtypes(include=np.number).columns if 'Label' not in c]\n",
    "        self.main_col = self.raw_cols[main_col_idx]\n",
    "        self.correlated_pairs = []\n",
    "\n",
    "        print(f\"Finding correlations relative to main column: '{self.main_col}'...\")\n",
    "        candidates = []\n",
    "        for col in self.raw_cols:\n",
    "            if col != self.main_col:\n",
    "                corr = df[self.main_col].corr(df[col])\n",
    "                if abs(corr) > 0.7:\n",
    "                    candidates.append(col)\n",
    "\n",
    "        for i, c1 in enumerate(candidates):\n",
    "            for c2 in candidates[i+1:]:\n",
    "                corr = df[c1].corr(df[c2])\n",
    "                if abs(corr) > 0.65:\n",
    "                    self.correlated_pairs.append((c1, c2))\n",
    "\n",
    "        print(f\"Found {len(self.correlated_pairs)} correlated pairs: {self.correlated_pairs}\")\n",
    "\n",
    "    def extract(self, df):\n",
    "        out = pd.DataFrame(index=df.index)\n",
    "        df_num = df[self.raw_cols]\n",
    "        col_0 = df_num[self.main_col]\n",
    "\n",
    "        # 1. Raw & DiffMain\n",
    "        out[self.main_col] = col_0\n",
    "        for col in self.raw_cols:\n",
    "            if col != self.main_col:\n",
    "                out[col] = df_num[col]\n",
    "                out[f'DiffMain_{col}'] = col_0 - df_num[col]\n",
    "\n",
    "        # 2. Speed\n",
    "        df_speed = df_num.diff()\n",
    "        for col in self.raw_cols:\n",
    "            out[f'speed_change_{col}'] = df_speed[col]\n",
    "\n",
    "        # 3. PRD\n",
    "        prev_0 = col_0.shift(1)\n",
    "        mean_val = (col_0 + prev_0) * 0.5\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            prd = abs(col_0 - prev_0) / mean_val\n",
    "        out['PRD'] = prd.fillna(0)\n",
    "\n",
    "        # 4. Pairs\n",
    "        for c1, c2 in self.correlated_pairs:\n",
    "            out[f'DiffPair_{c1}_{c2}'] = df_num[c1] - df_num[c2]\n",
    "\n",
    "        return out.fillna(0)\n",
    "\n",
    "# ================= C++ GENERATORS =================\n",
    "def generate_tsassure_settings(extractor, raw_cols):\n",
    "    lines = []\n",
    "    lines.append(f\"#define NUM_RAW_INPUTS {len(raw_cols)}\")\n",
    "    col_map = {name: i for i, name in enumerate(raw_cols)}\n",
    "    lines.append(f\"#define IDX_MAIN_COL {col_map[extractor.main_col]}\")\n",
    "    lines.append(f\"#define NUM_PAIRS {len(extractor.correlated_pairs)}\")\n",
    "    pairs_str = \"\"\n",
    "    for c1, c2 in extractor.correlated_pairs:\n",
    "        pairs_str += f\"  {{ {col_map[c1]}, {col_map[c2]} }}, // {c1}-{c2}\\n\"\n",
    "    return lines, pairs_str\n",
    "\n",
    "def export_lr_model(lr, scaler, prefix, filename):\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(f\"\\n// ===== MODEL: {prefix} (Logistic Regression) =====\\n\")\n",
    "        f.write(f\"#define {prefix}_N_FEATURES {scaler.n_features_in_}\\n\")\n",
    "        f.write(f\"static const float {prefix}_SCALE_MEAN[] = {{ {', '.join(f'{x:.6f}' for x in scaler.mean_)} }};\\n\")\n",
    "        f.write(f\"static const float {prefix}_SCALE_STD[]  = {{ {', '.join(f'{x:.6f}' for x in scaler.scale_)} }};\\n\")\n",
    "        coefs = lr.coef_[0]\n",
    "        f.write(f\"static const float {prefix}_COEF[] = {{ {', '.join(f'{x:.6f}' for x in coefs)} }};\\n\")\n",
    "        bias = lr.intercept_[0]\n",
    "        f.write(f\"static const float {prefix}_BIAS = {bias:.6f};\\n\")\n",
    "\n",
    "# ================= MAIN =================\n",
    "def main():\n",
    "    print(\"1. Loading Data...\")\n",
    "    dfs = []\n",
    "    for f in TRAIN_FILES:\n",
    "        try:\n",
    "            d = pd.read_excel(f)\n",
    "            d['Label'] = pd.to_numeric(d.get('Label', 1), errors='coerce').fillna(1)\n",
    "            dfs.append(d)\n",
    "        except: pass\n",
    "    if not dfs: return\n",
    "    df_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # --- TRAIN TSASSURE MODEL (LR) ---\n",
    "    print(\"2. Training TsAssure Model (LR with SMOTE)...\")\n",
    "    ext = TsAssureExtractor(df_all, main_col_idx=0)\n",
    "    X = ext.extract(df_all)\n",
    "    y = df_all['Label']\n",
    "\n",
    "    # SMOTE REQUIREMENT\n",
    "    sm = SMOTE(random_state=SEED)\n",
    "    X_res, y_res = sm.fit_resample(X, y)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_s = scaler.fit_transform(X_res)\n",
    "\n",
    "    lr = LogisticRegression(random_state=SEED, max_iter=1000)\n",
    "    lr.fit(X_s, y_res)\n",
    "\n",
    "    # --- EXPORT ---\n",
    "    print(\"3. Writing headers...\")\n",
    "    if os.path.exists(\"tsassure_settings.h\"): os.remove(\"tsassure_settings.h\")\n",
    "    if os.path.exists(\"model_edge.h\"): os.remove(\"model_edge.h\")\n",
    "\n",
    "    setting_lines, pairs_str = generate_tsassure_settings(ext, ext.raw_cols)\n",
    "\n",
    "    with open(\"tsassure_settings.h\", \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        for i, c in enumerate(ext.raw_cols): f.write(f\"#define IDX_{c.upper().replace(' ','_')} {i}\\n\")\n",
    "        for line in setting_lines: f.write(line + \"\\n\")\n",
    "        f.write(f\"\\nstatic const int CORR_PAIRS[NUM_PAIRS > 0 ? NUM_PAIRS : 1][2] = {{\\n{pairs_str}}};\\n\")\n",
    "        f.write(f\"\\n#define TS_N_FEATURES {X.shape[1]}\\n\")\n",
    "\n",
    "    with open(\"model_edge.h\", \"w\") as f:\n",
    "        f.write(\"#pragma once\\n#include <stdint.h>\\n\")\n",
    "        export_lr_model(lr, scaler, \"LR\", \"model_edge.h\")\n",
    "\n",
    "    print(f\"Done. Features: {X.shape[1]}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uaUM0fPusum"
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5CVC9aVEutb5",
    "outputId": "68380ed4-a115-4231-cdf2-cb8e56c228d3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import random\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "TRAIN_FILES = [\n",
    "    \"/content/dataset/Train-set_1.xlsx\",\n",
    "    \"/content/dataset/Train-set_2.xlsx\"\n",
    "]\n",
    "\n",
    "class TsAssureExtractor:\n",
    "    def __init__(self, df, main_col_idx=0):\n",
    "        self.raw_cols = [c for c in df.select_dtypes(include=np.number).columns if 'Label' not in c]\n",
    "        self.main_col = self.raw_cols[main_col_idx]\n",
    "        self.correlated_pairs = []\n",
    "\n",
    "        print(f\"Finding correlations relative to main column: '{self.main_col}'...\")\n",
    "        candidates = []\n",
    "        for col in self.raw_cols:\n",
    "            if col != self.main_col:\n",
    "                corr = df[self.main_col].corr(df[col])\n",
    "                if abs(corr) > 0.7:\n",
    "                    candidates.append(col)\n",
    "\n",
    "        for i, c1 in enumerate(candidates):\n",
    "            for c2 in candidates[i+1:]:\n",
    "                corr = df[c1].corr(df[c2])\n",
    "                if abs(corr) > 0.65:\n",
    "                    self.correlated_pairs.append((c1, c2))\n",
    "\n",
    "    def extract(self, df):\n",
    "        out = pd.DataFrame(index=df.index)\n",
    "        df_num = df[self.raw_cols]\n",
    "        col_0 = df_num[self.main_col]\n",
    "\n",
    "        # 1. Raw & DiffMain\n",
    "        out[self.main_col] = col_0\n",
    "        for col in self.raw_cols:\n",
    "            if col != self.main_col:\n",
    "                out[col] = df_num[col]\n",
    "                out[f'DiffMain_{col}'] = col_0 - df_num[col]\n",
    "\n",
    "        # 2. Speed\n",
    "        df_speed = df_num.diff()\n",
    "        for col in self.raw_cols:\n",
    "            out[f'speed_change_{col}'] = df_speed[col]\n",
    "\n",
    "        # 3. PRD\n",
    "        prev_0 = col_0.shift(1)\n",
    "        mean_val = (col_0 + prev_0) * 0.5\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            prd = abs(col_0 - prev_0) / mean_val\n",
    "        out['PRD'] = prd.fillna(0)\n",
    "\n",
    "        # 4. Pairs\n",
    "        for c1, c2 in self.correlated_pairs:\n",
    "            out[f'DiffPair_{c1}_{c2}'] = df_num[c1] - df_num[c2]\n",
    "\n",
    "        return out.fillna(0)\n",
    "\n",
    "def generate_tsassure_settings(extractor, raw_cols):\n",
    "    lines = []\n",
    "    lines.append(f\"#define NUM_RAW_INPUTS {len(raw_cols)}\")\n",
    "    col_map = {name: i for i, name in enumerate(raw_cols)}\n",
    "    lines.append(f\"#define IDX_MAIN_COL {col_map[extractor.main_col]}\")\n",
    "    lines.append(f\"#define NUM_PAIRS {len(extractor.correlated_pairs)}\")\n",
    "    pairs_str = \"\"\n",
    "    for c1, c2 in extractor.correlated_pairs:\n",
    "        pairs_str += f\"  {{ {col_map[c1]}, {col_map[c2]} }}, // {c1}-{c2}\\n\"\n",
    "    return lines, pairs_str\n",
    "\n",
    "def export_svm_model(svm, scaler, prefix, filename):\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(f\"\\n// ===== MODEL: {prefix} (Linear SVM) =====\\n\")\n",
    "        f.write(f\"#define {prefix}_N_FEATURES {scaler.n_features_in_}\\n\")\n",
    "        f.write(f\"static const float {prefix}_SCALE_MEAN[] = {{ {', '.join(f'{x:.6f}' for x in scaler.mean_)} }};\\n\")\n",
    "        f.write(f\"static const float {prefix}_SCALE_STD[]  = {{ {', '.join(f'{x:.6f}' for x in scaler.scale_)} }};\\n\")\n",
    "        coefs = svm.coef_[0]\n",
    "        f.write(f\"static const float {prefix}_COEF[] = {{ {', '.join(f'{x:.6f}' for x in coefs)} }};\\n\")\n",
    "        bias = svm.intercept_[0]\n",
    "        f.write(f\"static const float {prefix}_BIAS = {bias:.6f};\\n\")\n",
    "        pa = svm.probA_[0] if hasattr(svm, 'probA_') else 0.0\n",
    "        pb = svm.probB_[0] if hasattr(svm, 'probB_') else 0.0\n",
    "        f.write(f\"static const float {prefix}_PROB_A = {pa:.6f};\\n\")\n",
    "        f.write(f\"static const float {prefix}_PROB_B = {pb:.6f};\\n\")\n",
    "\n",
    "def main():\n",
    "    print(\"1. Loading Data...\")\n",
    "    dfs = []\n",
    "    for f in TRAIN_FILES:\n",
    "        try:\n",
    "            d = pd.read_excel(f)\n",
    "            d['Label'] = pd.to_numeric(d.get('Label', 1), errors='coerce').fillna(1)\n",
    "            dfs.append(d)\n",
    "        except: pass\n",
    "    if not dfs: return\n",
    "    df_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    print(\"2. Training TsAssure Model (SVM with SMOTE)...\")\n",
    "    ext = TsAssureExtractor(df_all, main_col_idx=0)\n",
    "    X = ext.extract(df_all)\n",
    "    y = df_all['Label']\n",
    "\n",
    "    sm = SMOTE(random_state=SEED)\n",
    "    X_res, y_res = sm.fit_resample(X, y)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_s = scaler.fit_transform(X_res)\n",
    "\n",
    "    svm = SVC(kernel='linear', probability=True, random_state=SEED)\n",
    "    svm.fit(X_s, y_res)\n",
    "\n",
    "    print(\"3. Writing headers...\")\n",
    "    if os.path.exists(\"tsassure_settings.h\"): os.remove(\"tsassure_settings.h\")\n",
    "    if os.path.exists(\"model_edge.h\"): os.remove(\"model_edge.h\")\n",
    "\n",
    "    setting_lines, pairs_str = generate_tsassure_settings(ext, ext.raw_cols)\n",
    "\n",
    "    with open(\"tsassure_settings.h\", \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        for i, c in enumerate(ext.raw_cols): f.write(f\"#define IDX_{c.upper().replace(' ','_')} {i}\\n\")\n",
    "        for line in setting_lines: f.write(line + \"\\n\")\n",
    "        f.write(f\"\\nstatic const int CORR_PAIRS[NUM_PAIRS > 0 ? NUM_PAIRS : 1][2] = {{\\n{pairs_str}}};\\n\")\n",
    "        f.write(f\"\\n#define TS_N_FEATURES {X.shape[1]}\\n\")\n",
    "\n",
    "    with open(\"model_edge.h\", \"w\") as f:\n",
    "        f.write(\"#pragma once\\n#include <stdint.h>\\n\")\n",
    "        export_svm_model(svm, scaler, \"SVM\", \"model_edge.h\")\n",
    "\n",
    "    print(f\"Done. Features: {X.shape[1]}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JGu71VD1_EY"
   },
   "source": [
    "# Hjorth Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cr663uWF2BzD"
   },
   "source": [
    "## LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tifpO37_2Abq",
    "outputId": "4f840dce-2d53-4d28-8688-2e57f97b7f38"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "TRAIN_FILES = [\n",
    "    \"/content/dataset/Train-set_1.xlsx\",\n",
    "    \"/content/dataset/Train-set_2.xlsx\"\n",
    "]\n",
    "WINDOW_SIZE = 10  # Window size for Hjorth parameter calculation\n",
    "\n",
    "# ================= HJORTH EXTRACTOR =================\n",
    "class HjorthExtractor:\n",
    "    def __init__(self, df):\n",
    "        # We process all numeric columns as independent sensors\n",
    "        self.raw_cols = [c for c in df.select_dtypes(include=np.number).columns if 'Label' not in c]\n",
    "        print(f\"Extracting Hjorth parameters for: {self.raw_cols}\")\n",
    "\n",
    "    def extract(self, df):\n",
    "        out = pd.DataFrame(index=df.index)\n",
    "\n",
    "        for col in self.raw_cols:\n",
    "            x = df[col]\n",
    "\n",
    "            # First Derivative (Velocity)\n",
    "            dx = x.diff()\n",
    "            # Second Derivative (Acceleration)\n",
    "            ddx = dx.diff()\n",
    "\n",
    "            # 1. Activity = Variance of the signal\n",
    "            activity = x.rolling(WINDOW_SIZE).var()\n",
    "\n",
    "            # Variance of derivatives\n",
    "            var_dx = dx.rolling(WINDOW_SIZE).var()\n",
    "            var_ddx = ddx.rolling(WINDOW_SIZE).var()\n",
    "\n",
    "            # 2. Mobility = sqrt(Var(dx) / Var(x))\n",
    "            # We use a small epsilon to avoid division by zero\n",
    "            mobility = np.sqrt(var_dx.div(activity + 1e-9))\n",
    "\n",
    "            # 3. Complexity = Mobility(dx) / Mobility(x)\n",
    "            # Mobility(dx) = sqrt(Var(ddx) / Var(dx))\n",
    "            mob_dx = np.sqrt(var_ddx.div(var_dx + 1e-9))\n",
    "            complexity = mob_dx.div(mobility + 1e-9)\n",
    "\n",
    "            out[f'Act_{col}'] = activity\n",
    "            out[f'Mob_{col}'] = mobility\n",
    "            out[f'Comp_{col}'] = complexity\n",
    "\n",
    "        return out.fillna(0)\n",
    "\n",
    "# ================= C++ GENERATORS =================\n",
    "def generate_hjorth_settings(extractor):\n",
    "    lines = []\n",
    "    lines.append(f\"#define NUM_RAW_INPUTS {len(extractor.raw_cols)}\")\n",
    "    lines.append(f\"#define HJORTH_WINDOW_SIZE {WINDOW_SIZE}\")\n",
    "    return lines\n",
    "\n",
    "def export_lr_model(lr, scaler, prefix, filename):\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(f\"\\n// ===== MODEL: {prefix} (Logistic Regression) =====\\n\")\n",
    "        f.write(f\"#define {prefix}_N_FEATURES {scaler.n_features_in_}\\n\")\n",
    "\n",
    "        f.write(f\"static const float {prefix}_SCALE_MEAN[] = {{ {', '.join(f'{x:.6f}' for x in scaler.mean_)} }};\\n\")\n",
    "        f.write(f\"static const float {prefix}_SCALE_STD[]  = {{ {', '.join(f'{x:.6f}' for x in scaler.scale_)} }};\\n\")\n",
    "\n",
    "        coefs = lr.coef_[0]\n",
    "        f.write(f\"static const float {prefix}_COEF[] = {{ {', '.join(f'{x:.6f}' for x in coefs)} }};\\n\")\n",
    "\n",
    "        bias = lr.intercept_[0]\n",
    "        f.write(f\"static const float {prefix}_BIAS = {bias:.6f};\\n\")\n",
    "\n",
    "# ================= MAIN =================\n",
    "def main():\n",
    "    print(\"1. Loading Data...\")\n",
    "    dfs = []\n",
    "    for f in TRAIN_FILES:\n",
    "        try:\n",
    "            d = pd.read_excel(f)\n",
    "            d['Label'] = pd.to_numeric(d.get('Label', 1), errors='coerce').fillna(1)\n",
    "            dfs.append(d)\n",
    "        except: pass\n",
    "    if not dfs: return\n",
    "    df_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # --- TRAIN HJORTH MODEL (LR with SMOTE) ---\n",
    "    print(\"2. Training Hjorth Model (LR + SMOTE)...\")\n",
    "    ext = HjorthExtractor(df_all)\n",
    "    X = ext.extract(df_all)\n",
    "    y = df_all['Label']\n",
    "\n",
    "    # Drop NaNs created by rolling window\n",
    "    mask = ~X.isna().any(axis=1)\n",
    "    X_clean = X[mask]\n",
    "    y_clean = y[mask]\n",
    "\n",
    "    # --- APPLY SMOTE ---\n",
    "    sm = SMOTE(random_state=SEED)\n",
    "    X_res, y_res = sm.fit_resample(X_clean, y_clean)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_s = scaler.fit_transform(X_res)\n",
    "\n",
    "    lr = LogisticRegression(random_state=SEED, max_iter=1000)\n",
    "    lr.fit(X_s, y_res)\n",
    "\n",
    "    # --- EXPORT ---\n",
    "    print(\"3. Writing headers...\")\n",
    "    if os.path.exists(\"hjorth_settings.h\"): os.remove(\"hjorth_settings.h\")\n",
    "    if os.path.exists(\"model_edge.h\"): os.remove(\"model_edge.h\")\n",
    "\n",
    "    setting_lines = generate_hjorth_settings(ext)\n",
    "\n",
    "    with open(\"hjorth_settings.h\", \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        for i, c in enumerate(ext.raw_cols): f.write(f\"#define IDX_{c.upper().replace(' ','_')} {i}\\n\")\n",
    "        for line in setting_lines: f.write(line + \"\\n\")\n",
    "        f.write(f\"\\n#define HJORTH_N_FEATURES {X.shape[1]}\\n\")\n",
    "\n",
    "    with open(\"model_edge.h\", \"w\") as f:\n",
    "        f.write(\"#pragma once\\n#include <stdint.h>\\n\")\n",
    "        export_lr_model(lr, scaler, \"LR\", \"model_edge.h\")\n",
    "\n",
    "    print(f\"Done. Features: {X.shape[1]}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWZG_B7Y8CCq"
   },
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "miZDIecp8Db-",
    "outputId": "1d2b8b40-d094-4c16-d58f-de71663ee324"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "TRAIN_FILES = [\n",
    "    \"/content/dataset/Train-set_1.xlsx\",\n",
    "    \"/content/dataset/Train-set_2.xlsx\"\n",
    "]\n",
    "WINDOW_SIZE = 10\n",
    "\n",
    "# ================= HJORTH EXTRACTOR =================\n",
    "class HjorthExtractor:\n",
    "    def __init__(self, df):\n",
    "        self.raw_cols = [c for c in df.select_dtypes(include=np.number).columns if 'Label' not in c]\n",
    "        print(f\"Extracting Hjorth parameters for: {self.raw_cols}\")\n",
    "\n",
    "    def extract(self, df):\n",
    "        out = pd.DataFrame(index=df.index)\n",
    "        for col in self.raw_cols:\n",
    "            x = df[col]\n",
    "            dx = x.diff()\n",
    "            ddx = dx.diff()\n",
    "\n",
    "            activity = x.rolling(WINDOW_SIZE).var()\n",
    "            var_dx = dx.rolling(WINDOW_SIZE).var()\n",
    "            var_ddx = ddx.rolling(WINDOW_SIZE).var()\n",
    "\n",
    "            mobility = np.sqrt(var_dx.div(activity + 1e-9))\n",
    "            mob_dx = np.sqrt(var_ddx.div(var_dx + 1e-9))\n",
    "            complexity = mob_dx.div(mobility + 1e-9)\n",
    "\n",
    "            out[f'Act_{col}'] = activity\n",
    "            out[f'Mob_{col}'] = mobility\n",
    "            out[f'Comp_{col}'] = complexity\n",
    "        return out.fillna(0)\n",
    "\n",
    "# ================= C++ GENERATORS =================\n",
    "def generate_hjorth_settings(extractor):\n",
    "    lines = []\n",
    "    lines.append(f\"#define NUM_RAW_INPUTS {len(extractor.raw_cols)}\")\n",
    "    lines.append(f\"#define HJORTH_WINDOW_SIZE {WINDOW_SIZE}\")\n",
    "    return lines\n",
    "\n",
    "def export_rf_model(rf, scaler, filename):\n",
    "    n_trees = len(rf.estimators_)\n",
    "    g_left, g_right, g_feature, g_threshold, g_value = [], [], [], [], []\n",
    "    tree_roots = []\n",
    "    offset = 0\n",
    "\n",
    "    for estimator in rf.estimators_:\n",
    "        tree = estimator.tree_\n",
    "        n_nodes = tree.node_count\n",
    "        tree_roots.append(offset)\n",
    "\n",
    "        t_left = tree.children_left\n",
    "        t_right = tree.children_right\n",
    "        t_feature = tree.feature\n",
    "        t_threshold = tree.threshold\n",
    "        t_value = tree.value[:, 0, :]\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            probs = t_value[:, 1] / t_value.sum(axis=1)\n",
    "            probs = np.nan_to_num(probs)\n",
    "\n",
    "        for i in range(n_nodes):\n",
    "            if t_left[i] != -1:\n",
    "                g_left.append(t_left[i] + offset)\n",
    "                g_right.append(t_right[i] + offset)\n",
    "            else:\n",
    "                g_left.append(-1)\n",
    "                g_right.append(-1)\n",
    "            g_feature.append(t_feature[i])\n",
    "            g_threshold.append(t_threshold[i])\n",
    "            g_value.append(probs[i])\n",
    "        offset += n_nodes\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"#pragma once\\n#include <stdint.h>\\n\\n\")\n",
    "        f.write(f\"// Random Forest: {n_trees} estimators\\n\")\n",
    "        f.write(f\"#define RF_N_FEATURES {scaler.n_features_in_}\\n\")\n",
    "        f.write(f\"static const float RF_SCALE_MEAN[] = {{ {', '.join(f'{x:.6f}' for x in scaler.mean_)} }};\\n\")\n",
    "        f.write(f\"static const float RF_SCALE_STD[]  = {{ {', '.join(f'{x:.6f}' for x in scaler.scale_)} }};\\n\\n\")\n",
    "        f.write(f\"#define RF_NUM_TREES {n_trees}\\n\")\n",
    "        f.write(f\"static const int RF_TREE_ROOTS[] = {{ {', '.join(map(str, tree_roots))} }};\\n\")\n",
    "        f.write(f\"static const int RF_LEFT[] = {{ {', '.join(map(str, g_left))} }};\\n\")\n",
    "        f.write(f\"static const int RF_RIGHT[] = {{ {', '.join(map(str, g_right))} }};\\n\")\n",
    "        f.write(f\"static const int RF_FEATURE[] = {{ {', '.join(map(str, g_feature))} }};\\n\")\n",
    "        f.write(f\"static const float RF_THRESHOLD[] = {{ {', '.join(f'{x:.6f}' for x in g_threshold)} }};\\n\")\n",
    "        f.write(f\"static const float RF_VALUE[] = {{ {', '.join(f'{x:.6f}' for x in g_value)} }};\\n\")\n",
    "\n",
    "# ================= MAIN =================\n",
    "def main():\n",
    "    print(\"1. Loading Data...\")\n",
    "    dfs = []\n",
    "    for f in TRAIN_FILES:\n",
    "        try:\n",
    "            d = pd.read_excel(f)\n",
    "            d['Label'] = pd.to_numeric(d.get('Label', 1), errors='coerce').fillna(1)\n",
    "            dfs.append(d)\n",
    "        except: pass\n",
    "    if not dfs: return\n",
    "    df_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    print(\"2. Training Hjorth Model (RF + SMOTE)...\")\n",
    "    ext = HjorthExtractor(df_all)\n",
    "    X = ext.extract(df_all)\n",
    "    y = df_all['Label']\n",
    "\n",
    "    mask = ~X.isna().any(axis=1)\n",
    "    X_clean = X[mask]\n",
    "    y_clean = y[mask]\n",
    "\n",
    "    # --- APPLY SMOTE ---\n",
    "    sm = SMOTE(random_state=SEED)\n",
    "    X_res, y_res = sm.fit_resample(X_clean, y_clean)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_s = scaler.fit_transform(X_res)\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=10, max_depth=8, random_state=SEED, n_jobs=1)\n",
    "    rf.fit(X_s, y_res)\n",
    "\n",
    "    print(\"3. Exporting...\")\n",
    "    if os.path.exists(\"hjorth_settings.h\"): os.remove(\"hjorth_settings.h\")\n",
    "    if os.path.exists(\"model_edge.h\"): os.remove(\"model_edge.h\")\n",
    "\n",
    "    setting_lines = generate_hjorth_settings(ext)\n",
    "\n",
    "    with open(\"hjorth_settings.h\", \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        for i, c in enumerate(ext.raw_cols): f.write(f\"#define IDX_{c.upper().replace(' ','_')} {i}\\n\")\n",
    "        for line in setting_lines: f.write(line + \"\\n\")\n",
    "        f.write(f\"\\n#define HJORTH_N_FEATURES {X.shape[1]}\\n\")\n",
    "\n",
    "    export_rf_model(rf, scaler, \"model_edge.h\")\n",
    "    print(f\"Done. Features: {X.shape[1]}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rnmEzV6BPbH"
   },
   "source": [
    "## SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ygGmgVb-8GW_",
    "outputId": "4f4bcbb2-6ffd-4892-fc72-cc17191ffb17"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import random\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "TRAIN_FILES = [\n",
    "    \"/content/dataset/Train-set_1.xlsx\",\n",
    "    \"/content/dataset/Train-set_2.xlsx\"\n",
    "]\n",
    "WINDOW_SIZE = 10\n",
    "\n",
    "# ================= HJORTH EXTRACTOR =================\n",
    "class HjorthExtractor:\n",
    "    def __init__(self, df):\n",
    "        self.raw_cols = [c for c in df.select_dtypes(include=np.number).columns if 'Label' not in c]\n",
    "        print(f\"Extracting Hjorth parameters for: {self.raw_cols}\")\n",
    "\n",
    "    def extract(self, df):\n",
    "        out = pd.DataFrame(index=df.index)\n",
    "        for col in self.raw_cols:\n",
    "            x = df[col]\n",
    "            dx = x.diff()\n",
    "            ddx = dx.diff()\n",
    "\n",
    "            activity = x.rolling(WINDOW_SIZE).var()\n",
    "            var_dx = dx.rolling(WINDOW_SIZE).var()\n",
    "            var_ddx = ddx.rolling(WINDOW_SIZE).var()\n",
    "\n",
    "            mobility = np.sqrt(var_dx.div(activity + 1e-9))\n",
    "            mob_dx = np.sqrt(var_ddx.div(var_dx + 1e-9))\n",
    "            complexity = mob_dx.div(mobility + 1e-9)\n",
    "\n",
    "            out[f'Act_{col}'] = activity\n",
    "            out[f'Mob_{col}'] = mobility\n",
    "            out[f'Comp_{col}'] = complexity\n",
    "        return out.fillna(0)\n",
    "\n",
    "# ================= C++ GENERATORS =================\n",
    "def generate_hjorth_settings(extractor):\n",
    "    lines = []\n",
    "    lines.append(f\"#define NUM_RAW_INPUTS {len(extractor.raw_cols)}\")\n",
    "    lines.append(f\"#define HJORTH_WINDOW_SIZE {WINDOW_SIZE}\")\n",
    "    return lines\n",
    "\n",
    "def export_svm_model(svm, scaler, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"#pragma once\\n#include <stdint.h>\\n\\n\")\n",
    "        f.write(f\"// Linear SVM Model\\n\")\n",
    "        f.write(f\"#define SVM_N_FEATURES {scaler.n_features_in_}\\n\")\n",
    "        f.write(f\"static const float SVM_SCALE_MEAN[] = {{ {', '.join(f'{x:.6f}' for x in scaler.mean_)} }};\\n\")\n",
    "        f.write(f\"static const float SVM_SCALE_STD[]  = {{ {', '.join(f'{x:.6f}' for x in scaler.scale_)} }};\\n\\n\")\n",
    "        coefs = svm.coef_[0]\n",
    "        f.write(f\"static const float SVM_COEF[] = {{ {', '.join(f'{x:.6f}' for x in coefs)} }};\\n\")\n",
    "        bias = svm.intercept_[0]\n",
    "        f.write(f\"static const float SVM_BIAS = {bias:.6f};\\n\")\n",
    "        # Assuming probability=True was not used for SVM in previous context or defaulting to linear decision\n",
    "        # If probability=True is needed like AD-FE, we need Platt parameters.\n",
    "        # But standard Linear SVM on MCUs usually outputs decision distance.\n",
    "        # AD-FE SVM uses Platt scaling. Let's stick to standard linear decision unless SVC(probability=True) is explicit.\n",
    "        # The prompt for Hjorth SVM uses SVC(kernel='linear', C=1.0).\n",
    "        # We will keep it simple: decision > 0 -> 1.\n",
    "\n",
    "# ================= MAIN =================\n",
    "def main():\n",
    "    print(\"1. Loading Data...\")\n",
    "    dfs = []\n",
    "    for f in TRAIN_FILES:\n",
    "        try:\n",
    "            d = pd.read_excel(f)\n",
    "            d['Label'] = pd.to_numeric(d.get('Label', 1), errors='coerce').fillna(1)\n",
    "            dfs.append(d)\n",
    "        except: pass\n",
    "    if not dfs: return\n",
    "    df_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    print(\"2. Training Hjorth Model (Linear SVM + SMOTE)...\")\n",
    "    ext = HjorthExtractor(df_all)\n",
    "    X = ext.extract(df_all)\n",
    "    y = df_all['Label']\n",
    "\n",
    "    mask = ~X.isna().any(axis=1)\n",
    "    X_clean = X[mask]\n",
    "    y_clean = y[mask]\n",
    "\n",
    "    # --- APPLY SMOTE ---\n",
    "    sm = SMOTE(random_state=SEED)\n",
    "    X_res, y_res = sm.fit_resample(X_clean, y_clean)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_s = scaler.fit_transform(X_res)\n",
    "\n",
    "    # Linear SVM\n",
    "    svm = SVC(kernel='linear', C=1.0, random_state=SEED)\n",
    "    svm.fit(X_s, y_res)\n",
    "\n",
    "    print(\"3. Exporting...\")\n",
    "    if os.path.exists(\"hjorth_settings.h\"): os.remove(\"hjorth_settings.h\")\n",
    "    if os.path.exists(\"model_edge.h\"): os.remove(\"model_edge.h\")\n",
    "\n",
    "    setting_lines = generate_hjorth_settings(ext)\n",
    "\n",
    "    with open(\"hjorth_settings.h\", \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        for i, c in enumerate(ext.raw_cols): f.write(f\"#define IDX_{c.upper().replace(' ','_')} {i}\\n\")\n",
    "        for line in setting_lines: f.write(line + \"\\n\")\n",
    "        f.write(f\"\\n#define HJORTH_N_FEATURES {X.shape[1]}\\n\")\n",
    "\n",
    "    export_svm_model(svm, scaler, \"model_edge.h\")\n",
    "    print(f\"Done. Features: {X.shape[1]}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SlYpN7k8HnbI"
   },
   "source": [
    "# catch 22\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Le7v_eItHoax",
    "outputId": "4ac698c3-3244-4450-da6c-7322069b9885"
   },
   "outputs": [],
   "source": [
    "!pip install pycatch22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMFhmP6EMGf3"
   },
   "source": [
    "## LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LZh2wXoSHu50",
    "outputId": "b2ece1af-c525-4d78-c645-b571f7e2a80c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import random\n",
    "try:\n",
    "    import pycatch22 as catch22\n",
    "except ImportError:\n",
    "    raise SystemExit(\"pycatch22 not found. Install with: pip install pycatch22\")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "TRAIN_FILES = [\n",
    "    \"/content/dataset/Train-set_1.xlsx\",\n",
    "    \"/content/dataset/Train-set_2.xlsx\"\n",
    "]\n",
    "WINDOW_SIZE = 40\n",
    "\n",
    "# The subset of features implemented in C++\n",
    "CATCH_LITE_FEATURES = [\n",
    "    \"DN_HistogramMode_5\",\n",
    "    \"DN_HistogramMode_10\",\n",
    "    \"CO_f1ecac\",\n",
    "    \"CO_FirstMin_ac\",\n",
    "    \"CO_trev_1_num\",\n",
    "    \"MD_hrv_classic_pnn40\"\n",
    "]\n",
    "\n",
    "# ================= CATCH22 EXTRACTOR =================\n",
    "class Catch22Extractor:\n",
    "    def __init__(self, df):\n",
    "        self.raw_cols = [c for c in df.select_dtypes(include=np.number).columns if 'Label' not in c]\n",
    "        print(f\"Extracting Catch22 (Lite) for: {self.raw_cols}\")\n",
    "\n",
    "    def extract(self, df):\n",
    "        out = pd.DataFrame(index=df.index)\n",
    "\n",
    "        for col in self.raw_cols:\n",
    "            series = df[col].to_numpy()\n",
    "            n_samples = len(series)\n",
    "\n",
    "            # Initialize arrays\n",
    "            feats = {f: np.zeros(n_samples) for f in CATCH_LITE_FEATURES}\n",
    "\n",
    "            # Simple rolling window loop\n",
    "            for i in range(WINDOW_SIZE, n_samples):\n",
    "                window = series[i-WINDOW_SIZE : i]\n",
    "                try:\n",
    "                    res = catch22.catch22_all(window)\n",
    "                    for f_name in CATCH_LITE_FEATURES:\n",
    "                        val = res['values'][res['names'].index(f_name)]\n",
    "                        feats[f_name][i] = val\n",
    "                except: pass\n",
    "\n",
    "            for f_name in CATCH_LITE_FEATURES:\n",
    "                out[f'{col}_{f_name}'] = feats[f_name]\n",
    "\n",
    "        return out.iloc[WINDOW_SIZE:].fillna(0)\n",
    "\n",
    "# ================= C++ GENERATORS =================\n",
    "def generate_catch22_settings(extractor):\n",
    "    lines = []\n",
    "    lines.append(f\"#define NUM_RAW_INPUTS {len(extractor.raw_cols)}\")\n",
    "    lines.append(f\"#define C22_WINDOW_SIZE {WINDOW_SIZE}\")\n",
    "    return lines\n",
    "\n",
    "def export_lr_model(lr, scaler, prefix, filename):\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(f\"\\n// ===== MODEL: {prefix} (Logistic Regression) =====\\n\")\n",
    "        f.write(f\"#define {prefix}_N_FEATURES {scaler.n_features_in_}\\n\")\n",
    "        f.write(f\"static const float {prefix}_SCALE_MEAN[] = {{ {', '.join(f'{x:.6f}' for x in scaler.mean_)} }};\\n\")\n",
    "        f.write(f\"static const float {prefix}_SCALE_STD[]  = {{ {', '.join(f'{x:.6f}' for x in scaler.scale_)} }};\\n\")\n",
    "        coefs = lr.coef_[0]\n",
    "        f.write(f\"static const float {prefix}_COEF[] = {{ {', '.join(f'{x:.6f}' for x in coefs)} }};\\n\")\n",
    "        bias = lr.intercept_[0]\n",
    "        f.write(f\"static const float {prefix}_BIAS = {bias:.6f};\\n\")\n",
    "\n",
    "# ================= MAIN =================\n",
    "def main():\n",
    "    print(\"1. Loading Data...\")\n",
    "    dfs = []\n",
    "    for f in TRAIN_FILES:\n",
    "        try:\n",
    "            d = pd.read_excel(f)\n",
    "            d['Label'] = pd.to_numeric(d.get('Label', 1), errors='coerce').fillna(1)\n",
    "            dfs.append(d)\n",
    "        except: pass\n",
    "    if not dfs: return\n",
    "    df_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    print(\"2. Training Catch22-Lite Model (LR with SMOTE)...\")\n",
    "    ext = Catch22Extractor(df_all)\n",
    "    X = ext.extract(df_all)\n",
    "    y = df_all['Label'].iloc[WINDOW_SIZE:] # Align y\n",
    "\n",
    "    mask = ~X.isna().any(axis=1)\n",
    "    X_clean = X[mask]\n",
    "    y_clean = y[mask]\n",
    "\n",
    "    # --- ADDED SMOTE ---\n",
    "    sm = SMOTE(random_state=SEED)\n",
    "    X_res, y_res = sm.fit_resample(X_clean, y_clean)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_s = scaler.fit_transform(X_res)\n",
    "\n",
    "    lr = LogisticRegression(random_state=SEED, max_iter=1000)\n",
    "    lr.fit(X_s, y_res)\n",
    "\n",
    "    print(\"3. Exporting...\")\n",
    "    if os.path.exists(\"catch22_settings.h\"): os.remove(\"catch22_settings.h\")\n",
    "    if os.path.exists(\"model_edge.h\"): os.remove(\"model_edge.h\")\n",
    "\n",
    "    setting_lines = generate_catch22_settings(ext)\n",
    "\n",
    "    with open(\"catch22_settings.h\", \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        for i, c in enumerate(ext.raw_cols): f.write(f\"#define IDX_{c.upper().replace(' ','_')} {i}\\n\")\n",
    "        for line in setting_lines: f.write(line + \"\\n\")\n",
    "        f.write(f\"\\n#define C22_N_FEATURES {X.shape[1]}\\n\")\n",
    "\n",
    "    with open(\"model_edge.h\", \"w\") as f:\n",
    "        f.write(\"#pragma once\\n#include <stdint.h>\\n\")\n",
    "        export_lr_model(lr, scaler, \"LR\", \"model_edge.h\")\n",
    "\n",
    "    print(f\"Done. Features: {X.shape[1]}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TiR7kqlBMJqe"
   },
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-yhUQPRPMKfm",
    "outputId": "bd5d0355-d8f0-4e09-fb3a-97fd5ce99348"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import random\n",
    "try:\n",
    "    import pycatch22 as catch22\n",
    "except ImportError:\n",
    "    raise SystemExit(\"pycatch22 not found. Install with: pip install pycatch22\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "TRAIN_FILES = [\n",
    "    \"/content/dataset/Train-set_1.xlsx\",\n",
    "    \"/content/dataset/Train-set_2.xlsx\"\n",
    "]\n",
    "WINDOW_SIZE = 40\n",
    "\n",
    "CATCH_LITE_FEATURES = [\n",
    "    \"DN_HistogramMode_5\",\n",
    "    \"DN_HistogramMode_10\",\n",
    "    \"CO_f1ecac\",\n",
    "    \"CO_FirstMin_ac\",\n",
    "    \"CO_trev_1_num\",\n",
    "    \"MD_hrv_classic_pnn40\"\n",
    "]\n",
    "\n",
    "class Catch22Extractor:\n",
    "    def __init__(self, df):\n",
    "        self.raw_cols = [c for c in df.select_dtypes(include=np.number).columns if 'Label' not in c]\n",
    "        print(f\"Extracting Catch22 (Lite) for: {self.raw_cols}\")\n",
    "\n",
    "    def extract(self, df):\n",
    "        out = pd.DataFrame(index=df.index)\n",
    "        for col in self.raw_cols:\n",
    "            series = df[col].to_numpy()\n",
    "            n_samples = len(series)\n",
    "            feats = {f: np.zeros(n_samples) for f in CATCH_LITE_FEATURES}\n",
    "\n",
    "            for i in range(WINDOW_SIZE, n_samples):\n",
    "                window = series[i-WINDOW_SIZE : i]\n",
    "                try:\n",
    "                    res = catch22.catch22_all(window)\n",
    "                    for f_name in CATCH_LITE_FEATURES:\n",
    "                        val = res['values'][res['names'].index(f_name)]\n",
    "                        feats[f_name][i] = val\n",
    "                except: pass\n",
    "\n",
    "            for f_name in CATCH_LITE_FEATURES:\n",
    "                out[f'{col}_{f_name}'] = feats[f_name]\n",
    "        return out.iloc[WINDOW_SIZE:].fillna(0)\n",
    "\n",
    "def generate_catch22_settings(extractor):\n",
    "    lines = []\n",
    "    lines.append(f\"#define NUM_RAW_INPUTS {len(extractor.raw_cols)}\")\n",
    "    lines.append(f\"#define C22_WINDOW_SIZE {WINDOW_SIZE}\")\n",
    "    return lines\n",
    "\n",
    "def export_rf_model(rf, scaler, filename):\n",
    "    n_trees = len(rf.estimators_)\n",
    "    g_left, g_right, g_feature, g_threshold, g_value = [], [], [], [], []\n",
    "    tree_roots = []\n",
    "    offset = 0\n",
    "\n",
    "    for estimator in rf.estimators_:\n",
    "        tree = estimator.tree_\n",
    "        n_nodes = tree.node_count\n",
    "        tree_roots.append(offset)\n",
    "\n",
    "        t_left = tree.children_left\n",
    "        t_right = tree.children_right\n",
    "        t_feature = tree.feature\n",
    "        t_threshold = tree.threshold\n",
    "        t_value = tree.value[:, 0, :]\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            probs = t_value[:, 1] / t_value.sum(axis=1)\n",
    "            probs = np.nan_to_num(probs)\n",
    "\n",
    "        for i in range(n_nodes):\n",
    "            if t_left[i] != -1:\n",
    "                g_left.append(t_left[i] + offset)\n",
    "                g_right.append(t_right[i] + offset)\n",
    "            else:\n",
    "                g_left.append(-1)\n",
    "                g_right.append(-1)\n",
    "            g_feature.append(t_feature[i])\n",
    "            g_threshold.append(t_threshold[i])\n",
    "            g_value.append(probs[i])\n",
    "        offset += n_nodes\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"#pragma once\\n#include <stdint.h>\\n\\n\")\n",
    "        f.write(f\"// Random Forest: {n_trees} estimators\\n\")\n",
    "        f.write(f\"#define RF_N_FEATURES {scaler.n_features_in_}\\n\")\n",
    "        f.write(f\"static const float RF_SCALE_MEAN[] = {{ {', '.join(f'{x:.6f}' for x in scaler.mean_)} }};\\n\")\n",
    "        f.write(f\"static const float RF_SCALE_STD[]  = {{ {', '.join(f'{x:.6f}' for x in scaler.scale_)} }};\\n\\n\")\n",
    "        f.write(f\"#define RF_NUM_TREES {n_trees}\\n\")\n",
    "        f.write(f\"static const int RF_TREE_ROOTS[] = {{ {', '.join(map(str, tree_roots))} }};\\n\")\n",
    "        f.write(f\"static const int RF_LEFT[] = {{ {', '.join(map(str, g_left))} }};\\n\")\n",
    "        f.write(f\"static const int RF_RIGHT[] = {{ {', '.join(map(str, g_right))} }};\\n\")\n",
    "        f.write(f\"static const int RF_FEATURE[] = {{ {', '.join(map(str, g_feature))} }};\\n\")\n",
    "        f.write(f\"static const float RF_THRESHOLD[] = {{ {', '.join(f'{x:.6f}' for x in g_threshold)} }};\\n\")\n",
    "        f.write(f\"static const float RF_VALUE[] = {{ {', '.join(f'{x:.6f}' for x in g_value)} }};\\n\")\n",
    "\n",
    "def main():\n",
    "    print(\"1. Loading Data...\")\n",
    "    dfs = []\n",
    "    for f in TRAIN_FILES:\n",
    "        try:\n",
    "            d = pd.read_excel(f)\n",
    "            d['Label'] = pd.to_numeric(d.get('Label', 1), errors='coerce').fillna(1)\n",
    "            dfs.append(d)\n",
    "        except: pass\n",
    "    if not dfs: return\n",
    "    df_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    print(\"2. Training Catch22-Lite Model (RF with SMOTE)...\")\n",
    "    ext = Catch22Extractor(df_all)\n",
    "    X = ext.extract(df_all)\n",
    "    y = df_all['Label'].iloc[WINDOW_SIZE:]\n",
    "\n",
    "    mask = ~X.isna().any(axis=1)\n",
    "    X_clean = X[mask]\n",
    "    y_clean = y[mask]\n",
    "\n",
    "    # --- ADDED SMOTE ---\n",
    "    sm = SMOTE(random_state=SEED)\n",
    "    X_res, y_res = sm.fit_resample(X_clean, y_clean)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_s = scaler.fit_transform(X_res)\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=10, max_depth=8, random_state=SEED, n_jobs=1)\n",
    "    rf.fit(X_s, y_res)\n",
    "\n",
    "    print(\"3. Exporting...\")\n",
    "    if os.path.exists(\"catch22_settings.h\"): os.remove(\"catch22_settings.h\")\n",
    "    if os.path.exists(\"model_edge.h\"): os.remove(\"model_edge.h\")\n",
    "\n",
    "    setting_lines = generate_catch22_settings(ext)\n",
    "\n",
    "    with open(\"catch22_settings.h\", \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        for i, c in enumerate(ext.raw_cols): f.write(f\"#define IDX_{c.upper().replace(' ','_')} {i}\\n\")\n",
    "        for line in setting_lines: f.write(line + \"\\n\")\n",
    "        f.write(f\"\\n#define C22_N_FEATURES {X.shape[1]}\\n\")\n",
    "\n",
    "    export_rf_model(rf, scaler, \"model_edge.h\")\n",
    "    print(f\"Done. Features: {X.shape[1]}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7YJlv3iRqp3"
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vuVCAdeFRrom",
    "outputId": "5f2cbf1d-9020-4f18-ebb7-6a6d752af6a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading Data...\n",
      "2. Training Catch22-Lite Model (SVM with SMOTE)...\n",
      "Extracting Catch22 (Lite) for: ['Temperature', 'Humidity', 'Temperature_WeatherStation', 'Humidity_WeatherStation']\n",
      "3. Exporting...\n",
      "Done. Features: 24\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import random\n",
    "try:\n",
    "    import pycatch22 as catch22\n",
    "except ImportError:\n",
    "    raise SystemExit(\"pycatch22 not found. Install with: pip install pycatch22\")\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "TRAIN_FILES = [\n",
    "    \"./dataset/Train-set_1.xlsx\",\n",
    "    \"./dataset/Train-set_2.xlsx\"\n",
    "]\n",
    "WINDOW_SIZE = 40\n",
    "\n",
    "CATCH_LITE_FEATURES = [\n",
    "    \"DN_HistogramMode_5\",\n",
    "    \"DN_HistogramMode_10\",\n",
    "    \"CO_f1ecac\",\n",
    "    \"CO_FirstMin_ac\",\n",
    "    \"CO_trev_1_num\",\n",
    "    \"MD_hrv_classic_pnn40\"\n",
    "]\n",
    "\n",
    "class Catch22Extractor:\n",
    "    def __init__(self, df):\n",
    "        self.raw_cols = [c for c in df.select_dtypes(include=np.number).columns if 'Label' not in c]\n",
    "        print(f\"Extracting Catch22 (Lite) for: {self.raw_cols}\")\n",
    "\n",
    "    def extract(self, df):\n",
    "        out = pd.DataFrame(index=df.index)\n",
    "        for col in self.raw_cols:\n",
    "            series = df[col].to_numpy()\n",
    "            n_samples = len(series)\n",
    "            feats = {f: np.zeros(n_samples) for f in CATCH_LITE_FEATURES}\n",
    "\n",
    "            for i in range(WINDOW_SIZE, n_samples):\n",
    "                window = series[i-WINDOW_SIZE : i]\n",
    "                try:\n",
    "                    res = catch22.catch22_all(window)\n",
    "                    for f_name in CATCH_LITE_FEATURES:\n",
    "                        val = res['values'][res['names'].index(f_name)]\n",
    "                        feats[f_name][i] = val\n",
    "                except: pass\n",
    "\n",
    "            for f_name in CATCH_LITE_FEATURES:\n",
    "                out[f'{col}_{f_name}'] = feats[f_name]\n",
    "        return out.iloc[WINDOW_SIZE:].fillna(0)\n",
    "\n",
    "def generate_catch22_settings(extractor):\n",
    "    lines = []\n",
    "    lines.append(f\"#define NUM_RAW_INPUTS {len(extractor.raw_cols)}\")\n",
    "    lines.append(f\"#define C22_WINDOW_SIZE {WINDOW_SIZE}\")\n",
    "    return lines\n",
    "\n",
    "def export_svm_model(svm, scaler, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"#pragma once\\n#include <stdint.h>\\n\\n\")\n",
    "        f.write(f\"// Linear SVM Model\\n\")\n",
    "        f.write(f\"#define SVM_N_FEATURES {scaler.n_features_in_}\\n\")\n",
    "        f.write(f\"static const float SVM_SCALE_MEAN[] = {{ {', '.join(f'{x:.6f}' for x in scaler.mean_)} }};\\n\")\n",
    "        f.write(f\"static const float SVM_SCALE_STD[]  = {{ {', '.join(f'{x:.6f}' for x in scaler.scale_)} }};\\n\\n\")\n",
    "        coefs = svm.coef_[0]\n",
    "        f.write(f\"static const float SVM_COEF[] = {{ {', '.join(f'{x:.6f}' for x in coefs)} }};\\n\")\n",
    "        bias = svm.intercept_[0]\n",
    "        f.write(f\"static const float SVM_BIAS = {bias:.6f};\\n\")\n",
    "\n",
    "def main():\n",
    "    print(\"1. Loading Data...\")\n",
    "    dfs = []\n",
    "    for f in TRAIN_FILES:\n",
    "        try:\n",
    "            d = pd.read_excel(f)\n",
    "            d['Label'] = pd.to_numeric(d.get('Label', 1), errors='coerce').fillna(1)\n",
    "            dfs.append(d)\n",
    "        except: pass\n",
    "    if not dfs: return\n",
    "    df_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    print(\"2. Training Catch22-Lite Model (SVM with SMOTE)...\")\n",
    "    ext = Catch22Extractor(df_all)\n",
    "    X = ext.extract(df_all)\n",
    "    y = df_all['Label'].iloc[WINDOW_SIZE:]\n",
    "\n",
    "    mask = ~X.isna().any(axis=1)\n",
    "    X_clean = X[mask]\n",
    "    y_clean = y[mask]\n",
    "\n",
    "    # --- ADDED SMOTE ---\n",
    "    sm = SMOTE(random_state=SEED)\n",
    "    X_res, y_res = sm.fit_resample(X_clean, y_clean)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_s = scaler.fit_transform(X_res)\n",
    "\n",
    "    svm = SVC(kernel='linear', C=1.0, random_state=SEED)\n",
    "    svm.fit(X_s, y_res)\n",
    "\n",
    "    print(\"3. Exporting...\")\n",
    "    if os.path.exists(\"catch22_settings.h\"): os.remove(\"catch22_settings.h\")\n",
    "    if os.path.exists(\"model_edge.h\"): os.remove(\"model_edge.h\")\n",
    "\n",
    "    setting_lines = generate_catch22_settings(ext)\n",
    "\n",
    "    with open(\"catch22_settings.h\", \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        for i, c in enumerate(ext.raw_cols): f.write(f\"#define IDX_{c.upper().replace(' ','_')} {i}\\n\")\n",
    "        for line in setting_lines: f.write(line + \"\\n\")\n",
    "        f.write(f\"\\n#define C22_N_FEATURES {X.shape[1]}\\n\")\n",
    "\n",
    "    export_svm_model(svm, scaler, \"model_edge.h\")\n",
    "    print(f\"Done. Features: {X.shape[1]}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "VLGD1q7Y3R4P"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "iaaa25-rfe-code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
